{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e05f769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import glob\n",
    "\n",
    "# # Define the folder path\n",
    "# computer_villa = 'C:/Users/rli04/Villanova University/Complete-trip-coordinate - Documents/General'\n",
    "# file_paths = glob.glob(computer_villa + '/Salt_Lake/delivery/Salt_Lake-Mar-2020/*.snappy.parquet')\n",
    "# df_list = [pd.read_parquet(file, engine='pyarrow') for file in file_paths]\n",
    "# # Load the first file\n",
    "# combined_df = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a1a94a",
   "metadata": {},
   "source": [
    "workflow\n",
    "1. raw data\n",
    "2. linked trip aggregation, sort by local_datetime_start\n",
    "3. select OD (airport to center city)[cences track level]\n",
    "4. linked trip filtter (the first trip O in the airport, the last trip D in the center city)\n",
    "5. attach geomery\n",
    "6. export csv (select linked trip with multi-modes or other filtter method)\n",
    "7. export json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf937a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "BASE_DIR = \"C:/Users/rli04/Villanova University/Complete-trip-coordinate - Documents/General\"\n",
    "PARQUET_DIR = f\"{BASE_DIR}/Salt_Lake/delivery\"\n",
    "TRACT_SHP = f\"{BASE_DIR}/Manuscript/Figure/Visualization-RL/2-OD patterns by census track/six_counties_track.shp\"\n",
    "\n",
    "# airport -> central city (example)\n",
    "ORIG_TRACT = \"49035980000\"#49057201900(weber)\n",
    "DEST_TRACT = \"49035114000\"\n",
    "\n",
    "MONTHS = ['Jan']\n",
    "        #'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "        #  'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "OUTPUT_CSV = \"air2central_complete_trip_samples.csv\"\n",
    "OUTPUT_JSON = \"samples_air2central.json\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7262213e",
   "metadata": {},
   "source": [
    "#### 1. Load raw parquet (minimal columns)\n",
    "#### 2. Clean + normalize (time, mode, distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49cda62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import pygeohash as pgh\n",
    "from shapely.geometry import Point, LineString\n",
    "from shapely import wkt\n",
    "import glob\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "USE_COLS = [\n",
    "    \"linked_trip_id\", \"trip_id\", \"tour_id\",\n",
    "    \"travel_mode\", \"local_datetime_start\", \"local_datetime_end\",\n",
    "    \"network_distance\", \"route_distance\",\n",
    "    \"geohash7_orig\", \"geohash7_dest\",\n",
    "    \"access_stop\", \"access_stop_id\",\n",
    "    \"egress_stop\", \"egress_stop_id\",\n",
    "    \"trip_purpose\", \"trip_weight\",\n",
    "    \"route_taken\"\n",
    "]\n",
    "\n",
    "dfs = []\n",
    "for m in MONTHS:\n",
    "    files = glob.glob(f\"{PARQUET_DIR}/Salt_Lake-{m}-2020/*.snappy.parquet\")\n",
    "    for f in files:\n",
    "        df = pd.read_parquet(f, columns=USE_COLS)\n",
    "        dfs.append(df)\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# basic clean\n",
    "df[\"local_datetime_start\"] = pd.to_datetime(df[\"local_datetime_start\"], errors=\"coerce\")\n",
    "df[\"local_datetime_end\"] = pd.to_datetime(df[\"local_datetime_end\"], errors=\"coerce\")\n",
    "df = df[df[\"local_datetime_end\"] > df[\"local_datetime_start\"]]\n",
    "\n",
    "df[\"duration_min\"] = (\n",
    "    df[\"local_datetime_end\"] - df[\"local_datetime_start\"]\n",
    ").dt.total_seconds() / 60\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddbd94b",
   "metadata": {},
   "source": [
    "#### 3. Geohash ‚Üí census tract (orig / dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c99013d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracts = gpd.read_file(TRACT_SHP).to_crs(\"EPSG:4326\")\n",
    "\n",
    "def gh_to_point(gh):\n",
    "    lat, lon = pgh.decode(gh)\n",
    "    return Point(lon, lat)\n",
    "\n",
    "gdf_o = gpd.GeoDataFrame(\n",
    "    df[[\"geohash7_orig\"]],\n",
    "    geometry=df[\"geohash7_orig\"].apply(gh_to_point),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "gdf_d = gpd.GeoDataFrame(\n",
    "    df[[\"geohash7_dest\"]],\n",
    "    geometry=df[\"geohash7_dest\"].apply(gh_to_point),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "df[\"GEOID_orig\"] = gpd.sjoin(gdf_o, tracts, how=\"left\", predicate=\"within\")[\"GEOID\"].values\n",
    "df[\"GEOID_dest\"] = gpd.sjoin(gdf_d, tracts, how=\"left\", predicate=\"within\")[\"GEOID\"].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c3038c",
   "metadata": {},
   "source": [
    "#### 4. Sort by linked_trip_id + time\n",
    "#### 5. Identify linked trips with:\n",
    "   - first.orig == airport tract\n",
    "   - last.dest == central tract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93c12ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values([\"linked_trip_id\", \"local_datetime_start\"])\n",
    "\n",
    "first = df.groupby(\"linked_trip_id\").first()\n",
    "last = df.groupby(\"linked_trip_id\").last()\n",
    "\n",
    "valid_linked = first[\n",
    "    (first[\"GEOID_orig\"] == ORIG_TRACT) &\n",
    "    (last[\"GEOID_dest\"] == DEST_TRACT)\n",
    "].index\n",
    "\n",
    "df = df[df[\"linked_trip_id\"].isin(valid_linked)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d14365f",
   "metadata": {},
   "source": [
    "#### 7. Attach network geometry (mode-aware)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94377eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load networks\n",
    "auto_links = pd.read_csv(f\"{BASE_DIR}/Salt_Lake/supplementInputs/network/auto-biggest-connected-graph/link.csv\")\n",
    "transit_links = pd.read_csv(f\"{BASE_DIR}/Salt_Lake/supplementInputs/network/UTA/link with flow.csv\")\n",
    "\n",
    "auto_dict = {\n",
    "    (int(r.from_osm_node_id), int(r.to_osm_node_id)): r.geometry\n",
    "    for r in auto_links.itertuples()\n",
    "}\n",
    "transit_dict = {\n",
    "    (int(r.from_node_id), int(r.to_node_id)): r.geometry\n",
    "    for r in transit_links.itertuples()\n",
    "}\n",
    "\n",
    "def build_geometry(row):\n",
    "    nodes = [int(x) for x in str(row.route_taken).split(\",\") if x.strip().isdigit()]\n",
    "    if len(nodes) < 2:\n",
    "        return None\n",
    "\n",
    "    coords = []\n",
    "    link_dict = auto_dict if row.travel_mode == \"car\" else transit_dict\n",
    "\n",
    "    for a, b in zip(nodes[:-1], nodes[1:]):\n",
    "        if (a, b) in link_dict:\n",
    "            try:\n",
    "                geom = wkt.loads(link_dict[(a, b)])\n",
    "                coords.extend(list(geom.coords))\n",
    "            except:\n",
    "                continue\n",
    "    return LineString(coords) if len(coords) > 1 else None\n",
    "\n",
    "df[\"geometry\"] = df.apply(build_geometry, axis=1)\n",
    "df = df[df[\"geometry\"].notnull()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975b7fc7",
   "metadata": {},
   "source": [
    "#### 8. Aggregate per segment (trip_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89ce8a3",
   "metadata": {},
   "source": [
    "#### 9. Export:\n",
    "   - CSV (debug / archive)\n",
    "   - JSON (dashboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59523236",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(OUTPUT_CSV, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2e6593d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def is_finite(x):\n",
    "    return x is not None and isinstance(x, (int, float)) and math.isfinite(x)\n",
    "\n",
    "def clean_num(x):\n",
    "    return float(x) if is_finite(x) else None\n",
    "\n",
    "def safe_decode_geohash(gh):\n",
    "    try:\n",
    "        lat, lon = pgh.decode(gh)\n",
    "        if is_finite(lat) and is_finite(lon):\n",
    "            return lon, lat\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None, None\n",
    "\n",
    "def build_route(geom):\n",
    "    if geom is None:\n",
    "        return None\n",
    "\n",
    "    coords = []\n",
    "    for lon, lat in geom.coords:\n",
    "        if not is_finite(lat) or not is_finite(lon):\n",
    "            continue\n",
    "        coords.append([float(lat), float(lon)])\n",
    "\n",
    "    if len(coords) < 2:\n",
    "        return None\n",
    "\n",
    "    # demo ÊäΩÁ®Ä\n",
    "    if len(coords) > 400:\n",
    "        coords = coords[::3]\n",
    "\n",
    "    return coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c834f3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "\n",
    "for r in df.itertuples():\n",
    "    route = build_route(r.geometry)\n",
    "    if route is None:\n",
    "        continue  # üö® ‰∏çÂêàÊ≥ïÁöÑÁõ¥Êé•‰∏¢ÂºÉ\n",
    "\n",
    "    o_lon, o_lat = safe_decode_geohash(r.geohash7_orig)\n",
    "    d_lon, d_lat = safe_decode_geohash(r.geohash7_dest)\n",
    "\n",
    "    sample = {\n",
    "        \"id\": str(r.trip_id),\n",
    "        \"mode\": str(r.travel_mode).lower().strip(),\n",
    "\n",
    "        \"route\": route,\n",
    "\n",
    "        \"duration_min\": clean_num(r.duration_min),\n",
    "        \"network_distance_km\": clean_num(r.network_distance),\n",
    "        \"route_distance_km\": clean_num(r.route_distance),\n",
    "\n",
    "        \"origin\": {\n",
    "            \"lon\": o_lon,\n",
    "            \"lat\": o_lat,\n",
    "            \"geohash\": r.geohash7_orig\n",
    "        },\n",
    "        \"destination\": {\n",
    "            \"lon\": d_lon,\n",
    "            \"lat\": d_lat,\n",
    "            \"geohash\": r.geohash7_dest\n",
    "        },\n",
    "\n",
    "        \"access\": {\n",
    "            \"stop_id\": clean_num(r.access_stop_id),\n",
    "            \"stop_name\": r.access_stop\n",
    "        },\n",
    "        \"egress\": {\n",
    "            \"stop_id\": clean_num(r.egress_stop_id),\n",
    "            \"stop_name\": r.egress_stop\n",
    "        },\n",
    "\n",
    "        \"meta\": {\n",
    "            \"linked_trip_id\": r.linked_trip_id,\n",
    "            \"tour_id\": r.tour_id,\n",
    "            \"purpose\": r.trip_purpose,\n",
    "            \"weight\": clean_num(r.trip_weight)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    samples.append(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24ca2fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = {\n",
    "    \"schema\": \"nova.complete_trip.sample.v1\",\n",
    "    \"generated_at\": datetime.utcnow().isoformat() + \"Z\",\n",
    "    \"count\": len(samples),\n",
    "    \"samples\": samples\n",
    "}\n",
    "\n",
    "with open(OUTPUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(out, f, indent=2, allow_nan=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa1a831",
   "metadata": {},
   "source": [
    "# old version below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddd22f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from datetime import timedelta\n",
    "import pygeohash as pgh\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Folder path\n",
    "folder_path = 'C:/Users/rli04/Villanova University/Complete-trip-coordinate - Documents/General'\n",
    "\n",
    "# Months\n",
    "months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "          'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "# Only load needed columns\n",
    "use_cols = [\n",
    "    'linked_trip_id', 'trip_id', 'travel_mode', 'local_datetime_start', 'local_datetime_end',\n",
    "    'network_distance', 'geohash7_orig', 'geohash7_dest', \n",
    "    'route_taken','route_distance', 'route_speed',\n",
    "    'access_stop_id','egress_stop_id',\n",
    "    'trip_purpose'\n",
    "]\n",
    "# Travel modes of interest\n",
    "modes = {'car', 'bus', 'rail', 'walk/bike'}  # use set for faster membership check\n",
    "\n",
    "# List to store DataFrames\n",
    "df_list = []\n",
    "\n",
    "for month in months:\n",
    "    files = glob.glob(f\"{folder_path}/Salt_Lake/delivery/Salt_Lake-{month}-2020/*.snappy.parquet\")\n",
    "    for file in files:\n",
    "        df = pd.read_parquet(file, engine='pyarrow', columns=use_cols)\n",
    "        df['linked_trip_id'] = df['linked_trip_id'].astype(str)\n",
    "        df['trip_id'] = df['trip_id'].astype(str)\n",
    "        df['travel_mode'] = df['travel_mode'].astype(str).str.lower().str.strip()\n",
    "\n",
    "        # 3. Êó∂Èó¥ÂàóËΩ¨‰∏∫ datetime\n",
    "        df['local_datetime_start'] = pd.to_datetime(df['local_datetime_start'], errors='coerce')\n",
    "        df['local_datetime_end'] = pd.to_datetime(df['local_datetime_end'], errors='coerce')\n",
    "\n",
    "        # 4. ËøáÊª§ÈùûÊ≥ïÊó∂Èó¥\n",
    "        df = df[df['local_datetime_end'] > df['local_datetime_start']]\n",
    "\n",
    "        # 5. ËÆ°ÁÆó duration\n",
    "        df['duration_minutes'] = (df['local_datetime_end'] - df['local_datetime_start']).dt.total_seconds() / 60\n",
    "\n",
    "        # 6. Ê∏ÖÊ¥óÁ©∫Èó¥ÂàóÔºàgeohashÔºâ\n",
    "        df['geohash7_orig'] = df['geohash7_orig'].astype(str).str.strip()\n",
    "        df['geohash7_dest'] = df['geohash7_dest'].astype(str).str.strip()\n",
    "\n",
    "        # 7. Ê∏ÖÊ¥óË∑ùÁ¶ªÂ≠óÊÆµÔºàÈÅøÂÖçÈùûÊï∞Â≠óÔºâ\n",
    "        df['network_distance'] = pd.to_numeric(df['network_distance'], errors='coerce')\n",
    "        df = df[df['network_distance'] > 0]\n",
    "        df_list.append(df)\n",
    "\n",
    "all_df = pd.concat(df_list, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1aada3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ 2. Âä†ËΩΩÂú∞ÁêÜËæπÁïåÂπ∂Á≠õÈÄâGeohashÂØπÂ∫îÂÖ≥Á≥ª ============\n",
    "tract_path = r\"C:/Users/rli04/Villanova University/Complete-trip-coordinate - Documents/General/Manuscript/Figure/Visualization-RL/2-OD patterns by census track/six_counties_track.shp\"\n",
    "tracts = gpd.read_file(tract_path).to_crs(\"EPSG:4326\")\n",
    "\n",
    "# Step 1: ÊûÑÈÄ† orig Âíå dest ÁÇπ\n",
    "gdf_orig = gpd.GeoDataFrame(\n",
    "    all_df[['geohash7_orig']],  # Âè™Áî® orig ÈÉ®ÂàÜ\n",
    "    geometry=all_df['geohash7_orig'].apply(lambda gh: Point(pgh.decode(gh)[1], pgh.decode(gh)[0])),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "gdf_orig.index = all_df.index  # ‰øùÊåÅ index ÂØπÈΩê\n",
    "\n",
    "gdf_dest = gpd.GeoDataFrame(\n",
    "    all_df[['geohash7_dest']],\n",
    "    geometry=all_df['geohash7_dest'].apply(lambda gh: Point(pgh.decode(gh)[1], pgh.decode(gh)[0])),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "gdf_dest.index = all_df.index\n",
    "\n",
    "# Step 2: Á©∫Èó¥ËøûÊé• tract shapefile\n",
    "gdf_orig_joined = gpd.sjoin(gdf_orig, tracts[['GEOID', 'geometry']], how=\"left\", predicate=\"within\")\n",
    "gdf_dest_joined = gpd.sjoin(gdf_dest, tracts[['GEOID', 'geometry']], how=\"left\", predicate=\"within\")\n",
    "\n",
    "# Step 3: Êää GEOID ‰ø°ÊÅØÈÄöËøá index merge Âõû all_df\n",
    "all_df['GEOID_orig'] = gdf_orig_joined['GEOID']\n",
    "all_df['GEOID_dest'] = gdf_dest_joined['GEOID']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c59e5cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import pygeohash as pgh\n",
    "from shapely.geometry import LineString\n",
    "import geopandas as gpd\n",
    "from datetime import datetime\n",
    "\n",
    "# Âä†ËΩΩÂπ∂Ê∏ÖÊ¥óÊï∞ÊçÆÔºöÁï•Ôºà‰ΩøÁî®‰Ω†Â∑≤ÊúâÁöÑ all_dfÔºâ\n",
    "\n",
    "# === Á¨¨‰∏ÄÊ≠•ÔºöÂåπÈÖç linked_trip Ëµ∑ÁÇπÁªàÁÇπ ===\n",
    "sorted_df = all_df.sort_values(['linked_trip_id', 'local_datetime_start'])\n",
    "\n",
    "first_trips = sorted_df.groupby('linked_trip_id').first().reset_index()\n",
    "last_trips = sorted_df.groupby('linked_trip_id').last().reset_index()\n",
    "\n",
    "merged = first_trips[['linked_trip_id', 'GEOID_orig']] \\\n",
    "    .merge(last_trips[['linked_trip_id', 'GEOID_dest']], on='linked_trip_id')\n",
    "\n",
    "target_linked = merged[\n",
    "    (merged['GEOID_orig'] == '49035114000') &\n",
    "    (merged['GEOID_dest'] == '49035110106')\n",
    "]['linked_trip_id'].unique()\n",
    "\n",
    "target_df = sorted_df[sorted_df['linked_trip_id'].isin(target_linked)]\n",
    "\n",
    "# === Á¨¨‰∫åÊ≠•ÔºöËØÜÂà´Á¨¶Âêà multimodal ÁöÑ trip ===\n",
    "def is_multimodal(modes):\n",
    "    seq = list(modes)\n",
    "    mode_set = set(seq)\n",
    "    # walk-bus-walk\n",
    "    # car + (bus or rail)\n",
    "    if 'car' in mode_set and 'walk/bike' in mode_set :\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "filtered_trips = []\n",
    "for ltid, group in target_df.groupby('linked_trip_id'):\n",
    "    modes = list(group.sort_values('local_datetime_start')['travel_mode'])\n",
    "    # if is_multimodal(modes):\n",
    "    filtered_trips.append(ltid)\n",
    "\n",
    "sample_df = target_df[target_df['linked_trip_id'].isin(filtered_trips)]\n",
    "\n",
    "# === Á¨¨‰∏âÊ≠•ÔºöÁîüÊàêÁªìÊûúË°® ===\n",
    "output = []\n",
    "for ltid, group in sample_df.groupby('linked_trip_id'):\n",
    "    group = group.sort_values('local_datetime_start')\n",
    "    mode_seq = '->'.join(group['travel_mode'])\n",
    "\n",
    "    # ÁªèÁ∫¨Â∫¶ÂùêÊ†áÂ∫èÂàó\n",
    "    coords = []\n",
    "    times = []\n",
    "    for _, row in group.iterrows():\n",
    "        try:\n",
    "            o_lat, o_lng = pgh.decode(row['geohash7_orig'])\n",
    "            d_lat, d_lng = pgh.decode(row['geohash7_dest'])\n",
    "            coords.append((o_lng, o_lat))  # shapely: (x, y) = (lon, lat)\n",
    "            coords.append((d_lng, d_lat))\n",
    "            times.append(row['local_datetime_start'])\n",
    "            times.append(row['local_datetime_end'])\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    if len(coords) < 2:\n",
    "        continue\n",
    "\n",
    "    linestring = LineString(coords)\n",
    "    times_str = '->'.join([t.strftime('%Y-%m-%d %H:%M:%S') for t in times])\n",
    "\n",
    "    output.append({\n",
    "        'linked_trip_id': ltid,\n",
    "        'mode_sequence': mode_seq,\n",
    "        'geometry_wkt': linestring.wkt,\n",
    "        'time_sequence': times_str\n",
    "    })\n",
    "\n",
    "# === Á¨¨ÂõõÊ≠•Ôºö‰øùÂ≠òÁªìÊûú ===\n",
    "output_df = pd.DataFrame(output)\n",
    "output_df.to_csv('multimodal_trip_sample.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6f0874",
   "metadata": {},
   "source": [
    "# specific sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bc0607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Done. File saved as selected_trips_with_geometry.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import pygeohash as pgh\n",
    "from shapely.geometry import LineString\n",
    "from shapely import wkt as shapely_wkt\n",
    "import ast\n",
    "\n",
    "# === ËÆæÁΩÆ ===\n",
    "folder_path = \"C:/Users/rli04/Villanova University/Complete-trip-coordinate - Documents/General\"\n",
    "target_ids = [\n",
    "    \"KWzJGrJMlbBXGBdYWd51rRYNrlVBRNYJQLwGZylWa4lZeKq0oP0LQE-rVzWQ05jPj2X9LLBQXpAvdqLrg5QX\",  # Jan\n",
    "    \"RLaKOp9YB2WDo44DoYLLKbeKjqzoWAz9J7vM7wEJxEDBwZExB262rO-1B1XElqEwldbAnxYQlwvwAqDx1xJg\",  # Feb\n",
    "]\n",
    "\n",
    "# === Âä†ËΩΩ Jan Âíå Feb ÁöÑÂÖ®ÈÉ®Êï∞ÊçÆ ===\n",
    "months = ['Jan', 'Feb']\n",
    "use_cols = ['linked_trip_id', 'trip_id', 'geohash7_orig', 'geohash7_dest', 'route_taken', 'travel_mode']\n",
    "df_list = []\n",
    "\n",
    "for month in months:\n",
    "    files = glob.glob(os.path.join(folder_path, f\"Salt_Lake/delivery/Salt_Lake-{month}-2020/*.snappy.parquet\"))\n",
    "    for file in files:\n",
    "        df = pd.read_parquet(file, engine='pyarrow', columns=use_cols)\n",
    "        df = df[df['linked_trip_id'].isin(target_ids)]\n",
    "        df_list.append(df)\n",
    "\n",
    "target_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# === ODÂùêÊ†áÂàó ===\n",
    "def decode_geohash_pair(row):\n",
    "    try:\n",
    "        lat1, lon1 = pgh.decode(row['geohash7_orig'])\n",
    "        lat2, lon2 = pgh.decode(row['geohash7_dest'])\n",
    "        return pd.Series([lon1, lat1, lon2, lat2])\n",
    "    except:\n",
    "        return pd.Series([None, None, None, None])\n",
    "\n",
    "target_df[['orig_lon', 'orig_lat', 'dest_lon', 'dest_lat']] = target_df.apply(decode_geohash_pair, axis=1)\n",
    "\n",
    "# === Âä†ËΩΩË∑ØÁΩëÂπ∂ÊûÑÂª∫ mode-specific Â≠óÂÖ∏ ===\n",
    "link_car = pd.read_csv(os.path.join(folder_path, \"Salt_Lake/supplementInputs/network/auto-biggest-connected-graph/link.csv\"))\n",
    "link_car_dict = {\n",
    "    (int(row['from_osm_node_id']), int(row['to_osm_node_id'])): row['geometry']\n",
    "    for _, row in link_car.iterrows()\n",
    "}\n",
    "\n",
    "link_transit = pd.read_csv(os.path.join(folder_path, \"Salt_Lake/supplementInputs/network/UTA/link with flow.csv\"))\n",
    "link_transit_dict = {\n",
    "    (int(row['from_node_id']), int(row['to_node_id'])): row['geometry']\n",
    "    for _, row in link_transit.iterrows()\n",
    "}\n",
    "\n",
    "# === ÊûÑÈÄ† trip-level Ê±áÊÄªÁªìÊûú ===\n",
    "output_rows = []\n",
    "for ltid, group in target_df.groupby('linked_trip_id'):\n",
    "    all_geoms = []\n",
    "    for _, row in group.iterrows():\n",
    "        try:\n",
    "            # Ëß£ÊûêËäÇÁÇπÂ∫èÂàó\n",
    "            nodes = [int(n.strip()) for n in row['route_taken'].split(',') if n.strip().isdigit() and int(n.strip()) != -1]\n",
    "            if len(nodes) < 2:\n",
    "                continue\n",
    "\n",
    "            # Ê†πÊçÆ mode ÈÄâÂèñ link_dict\n",
    "            mode = row['travel_mode'].lower()\n",
    "            if mode == 'car':\n",
    "                link_dict = link_car_dict\n",
    "            elif mode in ['bus', 'rail']:\n",
    "                link_dict = link_transit_dict\n",
    "            else:\n",
    "                continue  # skip walk/bike/air etc.\n",
    "\n",
    "            # Êü•Êâæ link geometry\n",
    "            for i in range(len(nodes)-1):\n",
    "                pair = (nodes[i], nodes[i+1])\n",
    "                if pair in link_dict:\n",
    "                    try:\n",
    "                        linestring = shapely_wkt.loads(link_dict[pair])\n",
    "                        all_geoms.extend(list(linestring.coords))\n",
    "                    except Exception as e:\n",
    "                        print(f\"‚ùå WKT parse error for pair {pair}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in trip {row['trip_id']}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if all_geoms:\n",
    "        full_linestring = LineString(all_geoms)\n",
    "        output_rows.append({\n",
    "            'linked_trip_id': ltid,\n",
    "            'trip_count': len(group),\n",
    "            'full_geometry_wkt': full_linestring.wkt\n",
    "        })\n",
    "\n",
    "# === ÂêàÂπ∂ÁªìÊûúÂπ∂‰øùÂ≠ò ===\n",
    "if output_rows:\n",
    "    output_df = pd.DataFrame(output_rows)\n",
    "    target_df = target_df.merge(output_df, on='linked_trip_id', how='left')\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No link geometries were matched. Adding empty column.\")\n",
    "    target_df['full_geometry_wkt'] = None\n",
    "\n",
    "target_df.to_csv(\"selected_trips_with_geometry.csv\", index=False)\n",
    "print(\"‚úÖ Done. File saved as selected_trips_with_geometry.csv.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadbe078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved to selected_linked_trips_jan_feb.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# ËÆæÁΩÆË∑ØÂæÑÂíå ID\n",
    "folder_path = \"C:/Users/rli04/Villanova University/Complete-trip-coordinate - Documents/General\"\n",
    "target_ids = [\n",
    "    \"KWzJGrJMlbBXGBdYWd51rRYNrlVBRNYJQLwGZylWa4lZeKq0oP0LQE-rVzWQ05jPj2X9LLBQXpAvdqLrg5QX\",  # Jan\n",
    "    \"RLaKOp9YB2WDo44DoYLLKbeKjqzoWAz9J7vM7wEJxEDBwZExB262rO-1B1XElqEwldbAnxYQlwvwAqDx1xJg\",  # Feb\n",
    "]\n",
    "months = ['Jan', 'Feb']\n",
    "\n",
    "# ËØªÂèñÂπ∂Á≠õÈÄâÊï∞ÊçÆ\n",
    "df_list = []\n",
    "for month in months:\n",
    "    files = glob.glob(os.path.join(folder_path, f\"Salt_Lake/delivery/Salt_Lake-{month}-2020/*.snappy.parquet\"))\n",
    "    for file in files:\n",
    "        df = pd.read_parquet(file)\n",
    "        filtered = df[df['linked_trip_id'].isin(target_ids)]\n",
    "        if not filtered.empty:\n",
    "            df_list.append(filtered)\n",
    "\n",
    "# ÂêàÂπ∂Âπ∂‰øùÂ≠ò\n",
    "if df_list:\n",
    "    result_df = pd.concat(df_list, ignore_index=True)\n",
    "    result_df.to_csv(\"selected_linked_trips_jan_feb.csv\", index=False)\n",
    "    print(\"‚úÖ Saved to selected_linked_trips_jan_feb.csv\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No matching linked_trip_id found in Jan or Feb.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
