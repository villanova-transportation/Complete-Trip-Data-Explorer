{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e05f769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import glob\n",
    "\n",
    "# # Define the folder path\n",
    "# computer_villa = 'C:/Users/rli04/Villanova University/Complete-trip-coordinate - Documents/General'\n",
    "# file_paths = glob.glob(computer_villa + '/Salt_Lake/delivery/Salt_Lake-Mar-2020/*.snappy.parquet')\n",
    "# df_list = [pd.read_parquet(file, engine='pyarrow') for file in file_paths]\n",
    "# # Load the first file\n",
    "# combined_df = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a1a94a",
   "metadata": {},
   "source": [
    "workflow\n",
    "1. raw data\n",
    "2. linked trip aggregation, sort by local_datetime_start\n",
    "3. select OD (airport to center city)[cences track level]\n",
    "4. linked trip filtter (the first trip O in the airport, the last trip D in the center city)\n",
    "5. attach geomery\n",
    "6. export csv (select linked trip with multi-modes or other filtter method)\n",
    "7. export json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf937a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "BASE_DIR = \"C:/Users/rli04/Villanova University/Complete-trip-coordinate - Documents/General\"\n",
    "PARQUET_DIR = f\"{BASE_DIR}/Salt_Lake/delivery\"\n",
    "TRACT_SHP = f\"{BASE_DIR}/Manuscript/Figure/Visualization-RL/2-OD patterns by census track/six_counties_track.shp\"\n",
    "\n",
    "# airport -> central city (example)\n",
    "ORIG_TRACT = \"49035980000\"#49057201900(weber)\n",
    "DEST_TRACT = \"49035114000\"\n",
    "\n",
    "MONTHS = ['Jan']\n",
    "        #'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "        #  'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "OUTPUT_CSV = \"air2central_complete_trip_samples.csv\"\n",
    "OUTPUT_JSON = \"samples_air2central.json\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7262213e",
   "metadata": {},
   "source": [
    "#### 1. Load raw parquet (minimal columns)\n",
    "#### 2. Clean + normalize (time, mode, distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49cda62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import pygeohash as pgh\n",
    "from shapely.geometry import Point, LineString\n",
    "from shapely import wkt\n",
    "import glob\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "USE_COLS = [\n",
    "    \"linked_trip_id\", \"trip_id\", \"tour_id\",\n",
    "    \"travel_mode\", \"local_datetime_start\", \"local_datetime_end\",\n",
    "    \"network_distance\", \"route_distance\",\n",
    "    \"geohash7_orig\", \"geohash7_dest\",\n",
    "    \"access_stop\", \"access_stop_id\",\n",
    "    \"egress_stop\", \"egress_stop_id\",\n",
    "    \"trip_purpose\", \"trip_weight\",\n",
    "    \"route_taken\"\n",
    "]\n",
    "\n",
    "dfs = []\n",
    "for m in MONTHS:\n",
    "    files = glob.glob(f\"{PARQUET_DIR}/Salt_Lake-{m}-2020/*.snappy.parquet\")\n",
    "    for f in files:\n",
    "        df = pd.read_parquet(f, columns=USE_COLS)\n",
    "        dfs.append(df)\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# basic clean\n",
    "df[\"local_datetime_start\"] = pd.to_datetime(df[\"local_datetime_start\"], errors=\"coerce\")\n",
    "df[\"local_datetime_end\"] = pd.to_datetime(df[\"local_datetime_end\"], errors=\"coerce\")\n",
    "df = df[df[\"local_datetime_end\"] > df[\"local_datetime_start\"]]\n",
    "\n",
    "df[\"duration_min\"] = (\n",
    "    df[\"local_datetime_end\"] - df[\"local_datetime_start\"]\n",
    ").dt.total_seconds() / 60\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddbd94b",
   "metadata": {},
   "source": [
    "#### 3. Geohash ‚Üí census tract (orig / dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c99013d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracts = gpd.read_file(TRACT_SHP).to_crs(\"EPSG:4326\")\n",
    "\n",
    "def gh_to_point(gh):\n",
    "    lat, lon = pgh.decode(gh)\n",
    "    return Point(lon, lat)\n",
    "\n",
    "gdf_o = gpd.GeoDataFrame(\n",
    "    df[[\"geohash7_orig\"]],\n",
    "    geometry=df[\"geohash7_orig\"].apply(gh_to_point),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "gdf_d = gpd.GeoDataFrame(\n",
    "    df[[\"geohash7_dest\"]],\n",
    "    geometry=df[\"geohash7_dest\"].apply(gh_to_point),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "df[\"GEOID_orig\"] = gpd.sjoin(gdf_o, tracts, how=\"left\", predicate=\"within\")[\"GEOID\"].values\n",
    "df[\"GEOID_dest\"] = gpd.sjoin(gdf_d, tracts, how=\"left\", predicate=\"within\")[\"GEOID\"].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c3038c",
   "metadata": {},
   "source": [
    "#### 4. Sort by linked_trip_id + time\n",
    "#### 5. Identify linked trips with:\n",
    "   - first.orig == airport tract\n",
    "   - last.dest == central tract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93c12ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values([\"linked_trip_id\", \"local_datetime_start\"])\n",
    "\n",
    "first = df.groupby(\"linked_trip_id\").first()\n",
    "last = df.groupby(\"linked_trip_id\").last()\n",
    "\n",
    "valid_linked = first[\n",
    "    (first[\"GEOID_orig\"] == ORIG_TRACT) &\n",
    "    (last[\"GEOID_dest\"] == DEST_TRACT)\n",
    "].index\n",
    "\n",
    "df = df[df[\"linked_trip_id\"].isin(valid_linked)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d14365f",
   "metadata": {},
   "source": [
    "#### 7. Attach network geometry (mode-aware)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94377eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load networks\n",
    "auto_links = pd.read_csv(f\"{BASE_DIR}/Salt_Lake/supplementInputs/network/auto-biggest-connected-graph/link.csv\")\n",
    "walk_links = pd.read_csv(f\"{BASE_DIR}/Salt_Lake/supplementInputs/network/walk-biggest-connected-graph/link.csv\")\n",
    "transit_links = pd.read_csv(f\"{BASE_DIR}/Salt_Lake/supplementInputs/network/UTA/link with flow.csv\")\n",
    "\n",
    "auto_dict = {\n",
    "    (int(r.from_osm_node_id), int(r.to_osm_node_id)): r.geometry\n",
    "    for r in auto_links.itertuples()\n",
    "}\n",
    "transit_dict = {\n",
    "    (int(r.from_node_id), int(r.to_node_id)): r.geometry\n",
    "    for r in transit_links.itertuples()\n",
    "}\n",
    "walk_dict = {\n",
    "    (int(r.from_osm_node_id), int(r.to_osm_node_id)): r.geometry\n",
    "    for r in walk_links.itertuples()\n",
    "}\n",
    "def build_geometry(row):\n",
    "    nodes = [int(x) for x in str(row.route_taken).split(\",\") if x.strip().isdigit()]\n",
    "    if len(nodes) < 2:\n",
    "        return None\n",
    "\n",
    "    coords = []\n",
    "    link_dict = (\n",
    "        auto_dict if row.travel_mode == \"car\"\n",
    "        else walk_dict if (row.travel_mode == \"walk/bike\")\n",
    "        else transit_dict if row.travel_mode in [\"bus\", \"rail\"]\n",
    "        else None\n",
    "    )\n",
    "    for a, b in zip(nodes[:-1], nodes[1:]):\n",
    "        if (a, b) in link_dict:\n",
    "            try:\n",
    "                geom = wkt.loads(link_dict[(a, b)])\n",
    "                coords.extend(list(geom.coords))\n",
    "            except:\n",
    "                continue\n",
    "    return LineString(coords) if len(coords) > 1 else None\n",
    "\n",
    "df[\"geometry\"] = df.apply(build_geometry, axis=1)\n",
    "df = df[df[\"geometry\"].notnull()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975b7fc7",
   "metadata": {},
   "source": [
    "#### 8. Aggregate per segment (trip_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89ce8a3",
   "metadata": {},
   "source": [
    "#### 9. Export:\n",
    "   - CSV (debug / archive)\n",
    "   - JSON (dashboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59523236",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(OUTPUT_CSV, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e741444",
   "metadata": {},
   "source": [
    "#### select sampls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "34fe87e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_mode(m):\n",
    "    if not isinstance(m, str):\n",
    "        return None\n",
    "    m = m.lower().strip()\n",
    "    if m == [\"car\"]:\n",
    "        return \"car\"\n",
    "    if m == \"bus\":\n",
    "        return \"bus\"\n",
    "    if m == [\"rail\"]:\n",
    "        return \"rail\"\n",
    "    if m == \"walk/bike\":\n",
    "        return \"walk/bike\"\n",
    "    return None\n",
    "\n",
    "df[\"mode_norm\"] = df[\"travel_mode\"].apply(normalize_mode)\n",
    "\n",
    "def is_valid_linked_trip(group):\n",
    "    modes = set(group[\"mode_norm\"].dropna())\n",
    "\n",
    "    # Êù°‰ª∂ 1ÔºöËá≥Â∞ë‰∏§Áßç mode\n",
    "    if len(modes) < 2:\n",
    "        return False\n",
    "\n",
    "    # Êù°‰ª∂ 2Ôºö‰∏çËÉΩÂè™Êúâ car\n",
    "    if modes == {\"car\"}:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "valid_linked_ids = (\n",
    "    df.groupby(\"linked_trip_id\")\n",
    "      .filter(is_valid_linked_trip)[\"linked_trip_id\"]\n",
    "      .unique()\n",
    ")\n",
    "\n",
    "df = df[df[\"linked_trip_id\"].isin(valid_linked_ids)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e2e6593d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def is_finite(x):\n",
    "    return x is not None and isinstance(x, (int, float)) and math.isfinite(x)\n",
    "\n",
    "def clean_num(x):\n",
    "    return float(x) if is_finite(x) else None\n",
    "\n",
    "def safe_decode_geohash(gh):\n",
    "    try:\n",
    "        lat, lon = pgh.decode(gh)\n",
    "        if is_finite(lat) and is_finite(lon):\n",
    "            return lon, lat\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None, None\n",
    "\n",
    "def build_route(geom):\n",
    "    if geom is None:\n",
    "        return None\n",
    "\n",
    "    coords = []\n",
    "    for lon, lat in geom.coords:\n",
    "        if not is_finite(lat) or not is_finite(lon):\n",
    "            continue\n",
    "        coords.append([float(lat), float(lon)])\n",
    "\n",
    "    if len(coords) < 2:\n",
    "        return None\n",
    "\n",
    "    # demo ÊäΩÁ®Ä\n",
    "    if len(coords) > 400:\n",
    "        coords = coords[::3]\n",
    "\n",
    "    return coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c834f3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "\n",
    "for r in df.itertuples():\n",
    "    route = build_route(r.geometry)\n",
    "    if route is None:\n",
    "        continue  # üö® ‰∏çÂêàÊ≥ïÁöÑÁõ¥Êé•‰∏¢ÂºÉ\n",
    "\n",
    "    o_lon, o_lat = safe_decode_geohash(r.geohash7_orig)\n",
    "    d_lon, d_lat = safe_decode_geohash(r.geohash7_dest)\n",
    "\n",
    "    sample = {\n",
    "        \"id\": str(r.trip_id),\n",
    "        \"mode\": str(r.travel_mode).lower().strip(),\n",
    "\n",
    "        \"route\": route,\n",
    "\n",
    "        \"duration_min\": clean_num(r.duration_min),\n",
    "        \"network_distance_km\": clean_num(r.network_distance),\n",
    "        \"route_distance_km\": clean_num(r.route_distance),\n",
    "\n",
    "        \"origin\": {\n",
    "            \"lon\": o_lon,\n",
    "            \"lat\": o_lat,\n",
    "            \"geohash\": r.geohash7_orig\n",
    "        },\n",
    "        \"destination\": {\n",
    "            \"lon\": d_lon,\n",
    "            \"lat\": d_lat,\n",
    "            \"geohash\": r.geohash7_dest\n",
    "        },\n",
    "\n",
    "        \"access\": {\n",
    "            \"stop_id\": clean_num(r.access_stop_id),\n",
    "            \"stop_name\": r.access_stop\n",
    "        },\n",
    "        \"egress\": {\n",
    "            \"stop_id\": clean_num(r.egress_stop_id),\n",
    "            \"stop_name\": r.egress_stop\n",
    "        },\n",
    "\n",
    "        \"meta\": {\n",
    "            \"linked_trip_id\": r.linked_trip_id,\n",
    "            \"tour_id\": r.tour_id,\n",
    "            \"purpose\": r.trip_purpose,\n",
    "            \"weight\": clean_num(r.trip_weight)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    samples.append(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "24ca2fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = {\n",
    "    \"schema\": \"nova.complete_trip.sample.v1\",\n",
    "    \"generated_at\": datetime.utcnow().isoformat() + \"Z\",\n",
    "    \"count\": len(samples),\n",
    "    \"samples\": samples\n",
    "}\n",
    "\n",
    "with open(OUTPUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(out, f, indent=2, allow_nan=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
