{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "6e05f769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import glob\n",
    "\n",
    "# # Define the folder path\n",
    "# computer_villa = 'C:/Users/rli04/Villanova University/Complete-trip-coordinate - Documents/General'\n",
    "# file_paths = glob.glob(computer_villa + '/Salt_Lake/delivery/Salt_Lake-Mar-2020/*.snappy.parquet')\n",
    "# df_list = [pd.read_parquet(file, engine='pyarrow') for file in file_paths]\n",
    "# # Load the first file\n",
    "# combined_df = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a1a94a",
   "metadata": {},
   "source": [
    "workflow\n",
    "1. raw data\n",
    "2. linked trip aggregation, sort by local_datetime_start\n",
    "3. select OD (airport to center city)[cences track level]\n",
    "4. linked trip filtter (the first trip O in the airport, the last trip D in the center city)\n",
    "5. attach geomery\n",
    "6. export csv (select linked trip with multi-modes or other filtter method)\n",
    "7. export json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "cf937a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "BASE_DIR = \"C:/Users/rli04/Villanova University/Complete-trip-coordinate - Documents/General\"\n",
    "PARQUET_DIR = f\"{BASE_DIR}/Salt_Lake/delivery\"\n",
    "TRACT_SHP = f\"{BASE_DIR}/Manuscript/Figure/Visualization-RL/2-OD patterns by census track/six_counties_track.shp\"\n",
    "\n",
    "# airport -> central city (example)\n",
    "ORIG_TRACT = \"49035101402\"#49057201900 (weber) \n",
    "DEST_TRACT = \"49035110106\" \n",
    "# 49035114000 (center)\n",
    "# 49035980000 (airport)\n",
    "# 49035110106 (Canyon)\n",
    "# 49035101402 (U of U)\n",
    "MONTHS = ['Jan']\n",
    "        #   'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "        #   'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "OUTPUT_CSV = f\"{ORIG_TRACT}_to_{DEST_TRACT}.csv\"\n",
    "OUTPUT_JSON = f\"{ORIG_TRACT}_to_{DEST_TRACT}.json\"\n",
    "\n",
    "# KEEP_FACTOR = 2.5   # OD < 2.5 * max(route_dist) -> DROP\n",
    "\n",
    "# MAX_SAMPLES = 3\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7262213e",
   "metadata": {},
   "source": [
    "#### 1. Load raw parquet (minimal columns)\n",
    "#### 2. Clean + normalize (time, mode, distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "49cda62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing month Jan ===\n",
      "Before OD filter, linked trips: 2751633\n",
      "Fixed OD distance (mile): 13.56\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pygeohash as pgh\n",
    "from shapely.geometry import Point, LineString\n",
    "from shapely import wkt\n",
    "import glob\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# =========================\n",
    "# UTILS\n",
    "# =========================\n",
    "def haversine_miles(lon1, lat1, lon2, lat2):\n",
    "    R = 3958.8\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2)**2\n",
    "    return 2 * R * np.arcsin(np.sqrt(a))\n",
    "\n",
    "def decode_gh_series(s):\n",
    "    lat, lon = zip(*s.map(pgh.decode))\n",
    "    return np.array(lat), np.array(lon)\n",
    "\n",
    "USE_COLS = [\n",
    "    \"linked_trip_id\", \"trip_id\", \"tour_id\",\n",
    "    \"travel_mode\", \"local_datetime_start\", \"local_datetime_end\",\n",
    "    \"network_distance\", \"route_distance\",\n",
    "    \"geohash7_orig\", \"geohash7_dest\",\n",
    "    \"access_stop\", \"access_stop_id\",\n",
    "    \"egress_stop\", \"egress_stop_id\",\n",
    "    \"trip_purpose\", \"trip_weight\",\n",
    "    \"route_taken\"\n",
    "]\n",
    "\n",
    "MONTHLY_DFS = []\n",
    "# =========================\n",
    "# 1️⃣ MONTHLY LOAD + BASIC FILTER\n",
    "# =========================\n",
    "for m in MONTHS:\n",
    "    print(f\"\\n=== Processing month {m} ===\")\n",
    "\n",
    "    files = glob.glob(f\"{PARQUET_DIR}/Salt_Lake-{m}-2020/*.snappy.parquet\")\n",
    "    if not files:\n",
    "        continue\n",
    "\n",
    "    dfs = [pd.read_parquet(f, columns=USE_COLS) for f in files]\n",
    "    df_month = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    df_month[\"local_datetime_start\"] = pd.to_datetime(df_month[\"local_datetime_start\"], errors=\"coerce\")\n",
    "    df_month[\"local_datetime_end\"] = pd.to_datetime(df_month[\"local_datetime_end\"], errors=\"coerce\")\n",
    "\n",
    "    df_month = df_month[df_month[\"local_datetime_end\"] > df_month[\"local_datetime_start\"]]\n",
    "\n",
    "    mode_sets = (\n",
    "        df_month\n",
    "        .groupby(\"linked_trip_id\")[\"travel_mode\"]\n",
    "        .agg(set)\n",
    "    )\n",
    "\n",
    "    # valid_linked_ids = mode_sets[\n",
    "    #     (mode_sets.apply(len) >= 2) &\n",
    "    #     (mode_sets != {\"car\"})\n",
    "    # ].index\n",
    "\n",
    "    # df_month = df_month[df_month[\"linked_trip_id\"].isin(valid_linked_ids)]\n",
    "    # ✅ ADD duration_min HERE (row-level, month scope)\n",
    "    df_month[\"duration_min\"] = (\n",
    "        df_month[\"local_datetime_end\"] - df_month[\"local_datetime_start\"]\n",
    "    ).dt.total_seconds() / 60\n",
    "\n",
    "    MONTHLY_DFS.append(df_month)\n",
    "\n",
    "# =========================\n",
    "# 2️⃣ YEAR CONCAT\n",
    "# =========================\n",
    "df = pd.concat(MONTHLY_DFS, ignore_index=True)\n",
    "print(\"Before OD filter, linked trips:\", df[\"linked_trip_id\"].nunique())\n",
    "# =========================\n",
    "# FIXED OD DISTANCE (TRACT CENTROID → CENTROID)\n",
    "# =========================\n",
    "tracts = gpd.read_file(TRACT_SHP).to_crs(\"EPSG:4326\")\n",
    "\n",
    "orig_geom = tracts.loc[\n",
    "    tracts[\"GEOID\"] == ORIG_TRACT, \"geometry\"\n",
    "].iloc[0]\n",
    "\n",
    "dest_geom = tracts.loc[\n",
    "    tracts[\"GEOID\"] == DEST_TRACT, \"geometry\"\n",
    "].iloc[0]\n",
    "\n",
    "orig_centroid = orig_geom.centroid\n",
    "dest_centroid = dest_geom.centroid\n",
    "\n",
    "OD_DIST_MILE = haversine_miles(\n",
    "    orig_centroid.x, orig_centroid.y,\n",
    "    dest_centroid.x, dest_centroid.y\n",
    ")\n",
    "\n",
    "print(f\"Fixed OD distance (mile): {OD_DIST_MILE:.2f}\")\n",
    "\n",
    "# =========================\n",
    "# 3️⃣ OD vs ROUTE DISTANCE FILTER（核心新增）\n",
    "# =========================\n",
    "df = df.sort_values([\"linked_trip_id\", \"local_datetime_start\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "4ed905c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After OD filter, linked trips: 2751633\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 使用 network_distance 作为每条 trip 的长度\n",
    "df[\"network_distance\"] = pd.to_numeric(\n",
    "    df[\"network_distance\"], errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# 对每个 linked_trip，取最长的一条 trip\n",
    "network_max = (\n",
    "    df[df[\"network_distance\"] >= 0]\n",
    "    .groupby(\"linked_trip_id\")[\"network_distance\"]\n",
    "    .max()\n",
    "    .rename(\"max_network_dist\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# 删除：只要有一条 trip > 2.5 * OD_DIST_MILE\n",
    "# valid_linked_ids = network_max.loc[\n",
    "#     network_max[\"max_network_dist\"] <= KEEP_FACTOR * OD_DIST_MILE,\n",
    "#     \"linked_trip_id\"\n",
    "# ]\n",
    "\n",
    "# df = df[df[\"linked_trip_id\"].isin(valid_linked_ids)]\n",
    "\n",
    "print(\"After OD filter, linked trips:\", df[\"linked_trip_id\"].nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddbd94b",
   "metadata": {},
   "source": [
    "#### 3. Geohash → census tract (orig / dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "c99013d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 4️⃣ TRACT JOIN（保持你原逻辑）\n",
    "# =========================\n",
    "tracts = gpd.read_file(TRACT_SHP).to_crs(\"EPSG:4326\")\n",
    "\n",
    "def gh_to_point(gh):\n",
    "    lat, lon = pgh.decode(gh)\n",
    "    return Point(lon, lat)\n",
    "\n",
    "gdf_o = gpd.GeoDataFrame(\n",
    "    df[[\"geohash7_orig\"]],\n",
    "    geometry=df[\"geohash7_orig\"].apply(gh_to_point),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "gdf_d = gpd.GeoDataFrame(\n",
    "    df[[\"geohash7_dest\"]],\n",
    "    geometry=df[\"geohash7_dest\"].apply(gh_to_point),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "df[\"GEOID_orig\"] = gpd.sjoin(gdf_o, tracts, how=\"left\", predicate=\"within\")[\"GEOID\"].values\n",
    "df[\"GEOID_dest\"] = gpd.sjoin(gdf_d, tracts, how=\"left\", predicate=\"within\")[\"GEOID\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c3038c",
   "metadata": {},
   "source": [
    "#### 4. Sort by linked_trip_id + time\n",
    "#### 5. Identify linked trips with:\n",
    "   - first.orig == airport tract\n",
    "   - last.dest == central tract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "93c12ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values([\"linked_trip_id\", \"local_datetime_start\"])\n",
    "\n",
    "first = df.groupby(\"linked_trip_id\").first()\n",
    "last = df.groupby(\"linked_trip_id\").last()\n",
    "\n",
    "valid_linked = first[\n",
    "    (first[\"GEOID_orig\"] == ORIG_TRACT) &\n",
    "    (last[\"GEOID_dest\"] == DEST_TRACT)\n",
    "].index\n",
    "\n",
    "df = df[df[\"linked_trip_id\"].isin(valid_linked)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d14365f",
   "metadata": {},
   "source": [
    "#### 7. Attach network geometry (mode-aware)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "94377eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load networks\n",
    "auto_links = pd.read_csv(f\"{BASE_DIR}/Salt_Lake/supplementInputs/network/auto-biggest-connected-graph/link.csv\")\n",
    "walk_links = pd.read_csv(f\"{BASE_DIR}/Salt_Lake/supplementInputs/network/walk-biggest-connected-graph/link.csv\")\n",
    "transit_links = pd.read_csv(f\"{BASE_DIR}/Salt_Lake/supplementInputs/network/UTA/link with flow.csv\")\n",
    "\n",
    "auto_dict = {\n",
    "    (int(r.from_osm_node_id), int(r.to_osm_node_id)): r.geometry\n",
    "    for r in auto_links.itertuples()\n",
    "}\n",
    "transit_dict = {\n",
    "    (int(r.from_node_id), int(r.to_node_id)): r.geometry\n",
    "    for r in transit_links.itertuples()\n",
    "}\n",
    "walk_dict = {\n",
    "    (int(r.from_osm_node_id), int(r.to_osm_node_id)): r.geometry\n",
    "    for r in walk_links.itertuples()\n",
    "}\n",
    "def build_geometry(row):\n",
    "    nodes = [int(x) for x in str(row.route_taken).split(\",\") if x.strip().isdigit()]\n",
    "    if len(nodes) < 2:\n",
    "        return None\n",
    "\n",
    "    coords = []\n",
    "    link_dict = (\n",
    "        auto_dict if row.travel_mode == \"car\"\n",
    "        else walk_dict if (row.travel_mode == \"walk/bike\")\n",
    "        else transit_dict if row.travel_mode in [\"bus\", \"rail\"]\n",
    "        else None\n",
    "    )\n",
    "    for a, b in zip(nodes[:-1], nodes[1:]):\n",
    "        if (a, b) in link_dict:\n",
    "            try:\n",
    "                geom = wkt.loads(link_dict[(a, b)])\n",
    "                coords.extend(list(geom.coords))\n",
    "            except:\n",
    "                continue\n",
    "    return LineString(coords) if len(coords) > 1 else None\n",
    "\n",
    "df[\"geometry\"] = df.apply(build_geometry, axis=1)\n",
    "df = df[df[\"geometry\"].notnull()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975b7fc7",
   "metadata": {},
   "source": [
    "#### 8. Aggregate per segment (trip_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89ce8a3",
   "metadata": {},
   "source": [
    "#### 9. Export:\n",
    "   - CSV (debug / archive)\n",
    "   - JSON (dashboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "59523236",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(OUTPUT_CSV, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "e2e6593d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def is_finite(x):\n",
    "    return x is not None and isinstance(x, (int, float)) and math.isfinite(x)\n",
    "\n",
    "def clean_num(x):\n",
    "    return float(x) if is_finite(x) else None\n",
    "\n",
    "def safe_decode_geohash(gh):\n",
    "    try:\n",
    "        lat, lon = pgh.decode(gh)\n",
    "        if is_finite(lat) and is_finite(lon):\n",
    "            return lon, lat\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None, None\n",
    "\n",
    "def build_route(geom):\n",
    "    if geom is None:\n",
    "        return None\n",
    "\n",
    "    coords = []\n",
    "    for lon, lat in geom.coords:\n",
    "        if not is_finite(lat) or not is_finite(lon):\n",
    "            continue\n",
    "        coords.append([float(lat), float(lon)])\n",
    "\n",
    "    if len(coords) < 2:\n",
    "        return None\n",
    "\n",
    "    # demo 抽稀\n",
    "    if len(coords) > 400:\n",
    "        coords = coords[::3]\n",
    "\n",
    "    return coords\n",
    "def to_iso(t):\n",
    "    if t is None:\n",
    "        return None\n",
    "    if hasattr(t, \"isoformat\"):\n",
    "        return t.isoformat()\n",
    "    return str(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "c834f3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict\n",
    "from shapely.geometry import mapping\n",
    "\n",
    "MAX_SAMPLES = 5\n",
    "\n",
    "# =========================\n",
    "# 1️⃣ BUILD ALL LEG SAMPLES（仅补 end_time）\n",
    "# =========================\n",
    "\n",
    "samples = []\n",
    "\n",
    "for r in df.itertuples():\n",
    "    route = build_route(r.geometry)\n",
    "    if route is None:\n",
    "        continue\n",
    "\n",
    "    o_lon, o_lat = safe_decode_geohash(r.geohash7_orig)\n",
    "    d_lon, d_lat = safe_decode_geohash(r.geohash7_dest)\n",
    "\n",
    "    start_dt = r.local_datetime_start\n",
    "    duration = clean_num(r.duration_min)\n",
    "\n",
    "    end_dt = (\n",
    "        start_dt + timedelta(minutes=duration)\n",
    "        if start_dt is not None and duration is not None\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    samples.append({\n",
    "        \"id\": str(r.trip_id),\n",
    "        \"mode\": str(r.travel_mode).lower().strip(),\n",
    "        \"route\": route,\n",
    "        \"start_time\": to_iso(start_dt),\n",
    "        \"end_time\": to_iso(end_dt),              # ✅ 新增\n",
    "        \"duration_min\": duration,\n",
    "        \"network_distance_km\": clean_num(r.network_distance),\n",
    "        \"route_distance_km\": clean_num(r.route_distance),\n",
    "        \"origin\": {\n",
    "            \"lon\": o_lon,\n",
    "            \"lat\": o_lat,\n",
    "            \"geohash\": r.geohash7_orig\n",
    "        },\n",
    "        \"destination\": {\n",
    "            \"lon\": d_lon,\n",
    "            \"lat\": d_lat,\n",
    "            \"geohash\": r.geohash7_dest\n",
    "        },\n",
    "        \"access\": {\n",
    "            \"stop_id\": clean_num(r.access_stop_id),\n",
    "            \"stop_name\": r.access_stop\n",
    "        },\n",
    "        \"egress\": {\n",
    "            \"stop_id\": clean_num(r.egress_stop_id),\n",
    "            \"stop_name\": r.egress_stop\n",
    "        },\n",
    "        \"meta\": {\n",
    "            \"linked_trip_id\": r.linked_trip_id,\n",
    "            \"tour_id\": r.tour_id,\n",
    "            \"purpose\": r.trip_purpose,\n",
    "            \"weight\": clean_num(r.trip_weight)\n",
    "        }\n",
    "    })\n",
    "\n",
    "# =========================\n",
    "# 2️⃣ GROUP BY linked_trip（不变）\n",
    "# =========================\n",
    "\n",
    "linked_groups = defaultdict(list)\n",
    "for s in samples:\n",
    "    linked_groups[s[\"meta\"][\"linked_trip_id\"]].append(s)\n",
    "\n",
    "# =========================\n",
    "# 3️⃣ BUILD FULL linked_trip STRUCTURE（仅修 end_time 语义）\n",
    "# =========================\n",
    "\n",
    "linked_trips_full = []\n",
    "\n",
    "for linked_id, trips in linked_groups.items():\n",
    "\n",
    "    trips_sorted = sorted(trips, key=lambda x: x[\"start_time\"])\n",
    "\n",
    "    # 给每个 leg 顺序索引\n",
    "    for i, t in enumerate(trips_sorted):\n",
    "        t[\"leg_index\"] = i\n",
    "\n",
    "    origin = {\n",
    "        **trips_sorted[0][\"origin\"],\n",
    "        \"start_time\": trips_sorted[0][\"start_time\"]\n",
    "    }\n",
    "\n",
    "    destination = {\n",
    "        **trips_sorted[-1][\"destination\"],\n",
    "        \"end_time\": trips_sorted[-1].get(\"end_time\")   # ✅ 修正\n",
    "    }\n",
    "\n",
    "    transfers = []\n",
    "    for t in trips_sorted[:-1]:\n",
    "        if t[\"destination\"][\"lat\"] is not None and t[\"destination\"][\"lon\"] is not None:\n",
    "            transfers.append({\n",
    "                \"lat\": t[\"destination\"][\"lat\"],\n",
    "                \"lon\": t[\"destination\"][\"lon\"],\n",
    "                \"geohash\": t[\"destination\"][\"geohash\"]\n",
    "            })\n",
    "\n",
    "    weight = max(t[\"meta\"][\"weight\"] or 0 for t in trips_sorted)\n",
    "\n",
    "    linked_trips_full.append({\n",
    "        \"linked_trip_id\": linked_id,\n",
    "        \"origin\": origin,\n",
    "        \"destination\": destination,\n",
    "        \"transfers\": transfers,\n",
    "        \"legs\": trips_sorted,\n",
    "        \"weight\": weight\n",
    "    })\n",
    "\n",
    "# =========================\n",
    "# 4️⃣ SAMPLE linked_trip（不变）\n",
    "# =========================\n",
    "\n",
    "linked_trips_sorted = sorted(\n",
    "    linked_trips_full,\n",
    "    key=lambda lt: -lt[\"weight\"]\n",
    ")\n",
    "\n",
    "linked_trips_final = linked_trips_sorted\n",
    "\n",
    "# =========================\n",
    "# 5️⃣ OD TRACT → GEOJSON（不变）\n",
    "# =========================\n",
    "\n",
    "tracts[\"GEOID\"] = tracts[\"GEOID\"].astype(str)\n",
    "\n",
    "origin_tract = tracts.loc[tracts[\"GEOID\"] == ORIG_TRACT]\n",
    "dest_tract   = tracts.loc[tracts[\"GEOID\"] == DEST_TRACT]\n",
    "\n",
    "if len(origin_tract) != 1 or len(dest_tract) != 1:\n",
    "    raise ValueError(\"OD tract not uniquely identified\")\n",
    "\n",
    "def geom_to_geojson(gdf):\n",
    "    return mapping(gdf.geometry.iloc[0])\n",
    "\n",
    "od_info = {\n",
    "    \"origin\": {\n",
    "        \"tract_id\": ORIG_TRACT,\n",
    "        \"geometry\": geom_to_geojson(origin_tract)\n",
    "    },\n",
    "    \"destination\": {\n",
    "        \"tract_id\": DEST_TRACT,\n",
    "        \"geometry\": geom_to_geojson(dest_tract)\n",
    "    }\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# 6️⃣ FINAL OUTPUT（不变）\n",
    "# =========================\n",
    "\n",
    "out = {\n",
    "    \"schema\": \"nova.complete_trip.sample.v2\",\n",
    "    \"generated_at\": datetime.utcnow().isoformat() + \"Z\",\n",
    "    \"od\": od_info,\n",
    "    \"count\": len(linked_trips_final),\n",
    "    \"linked_trips\": linked_trips_final\n",
    "}\n",
    "\n",
    "with open(OUTPUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(out, f, indent=2, allow_nan=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
