{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58e810c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 49035114000 (center)\n",
    "# 49035980000 (airport)\n",
    "# 49035110106 (ski)\n",
    "# 49035101402 (U of U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879c3751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FAST + ROBUST VERSION\n",
    "# Build OD-level annual statistics for Complete Trips\n",
    "# OD defined strictly by Census Tract (GEOID + shapefile)\n",
    "# Population-level statistics (NOT sampled)\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import glob\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pygeohash as pgh\n",
    "\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "\n",
    "BASE_DIR = \"C:/Users/rli04/Villanova University/Complete-trip-coordinate - Documents/General\"\n",
    "PARQUET_DIR = f\"{BASE_DIR}/Salt_Lake/delivery\"\n",
    "\n",
    "TRACT_SHP = (\n",
    "    f\"{BASE_DIR}/Manuscript/Figure/Visualization-RL/\"\n",
    "    f\"2-OD patterns by census track/six_counties_track.shp\"\n",
    ")\n",
    "\n",
    "ORIG_TRACT = \"49035110106\"\n",
    "DEST_TRACT = \"49035114000\"\n",
    "# 49035114000 (center)\n",
    "# 49035980000 (airport)\n",
    "# 49035110106 (ski)\n",
    "# 49035101402 (U of U)\n",
    "MONTHS = [\n",
    "    \"Jan\"\n",
    "    # ,\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\n",
    "    # \"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"\n",
    "]\n",
    "\n",
    "OUTPUT_STATS_JSON = f\"{ORIG_TRACT}_to_{DEST_TRACT}.stats.json\"\n",
    "\n",
    "USE_COLS = [\n",
    "    \"linked_trip_id\",\n",
    "    \"travel_mode\",\n",
    "    \"local_datetime_start\",\n",
    "    \"local_datetime_end\",\n",
    "    \"geohash7_orig\",\n",
    "    \"geohash7_dest\"\n",
    "]\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 0️⃣ LOAD TRACTS\n",
    "# =========================\n",
    "\n",
    "tracts = gpd.read_file(TRACT_SHP).to_crs(\"EPSG:4326\")\n",
    "tracts[\"GEOID\"] = tracts[\"GEOID\"].astype(str)\n",
    "\n",
    "orig_poly = tracts.loc[tracts[\"GEOID\"] == ORIG_TRACT, [\"geometry\"]].copy()\n",
    "dest_poly = tracts.loc[tracts[\"GEOID\"] == DEST_TRACT, [\"geometry\"]].copy()\n",
    "\n",
    "if orig_poly.empty or dest_poly.empty:\n",
    "    raise ValueError(\"Origin/Destination tract GEOID not found in shapefile.\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 1️⃣ LOAD YEARLY DATA\n",
    "# =========================\n",
    "\n",
    "dfs = []\n",
    "for m in MONTHS:\n",
    "    files = glob.glob(f\"{PARQUET_DIR}/Salt_Lake-{m}-2020/*.snappy.parquet\")\n",
    "    if not files:\n",
    "        continue\n",
    "    dfs.append(pd.concat(\n",
    "        [pd.read_parquet(f, columns=USE_COLS) for f in files],\n",
    "        ignore_index=True\n",
    "    ))\n",
    "\n",
    "if not dfs:\n",
    "    raise ValueError(\"No parquet files found. Check PARQUET_DIR and MONTHS.\")\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "df[\"local_datetime_start\"] = pd.to_datetime(df[\"local_datetime_start\"], errors=\"coerce\")\n",
    "df[\"local_datetime_end\"]   = pd.to_datetime(df[\"local_datetime_end\"], errors=\"coerce\")\n",
    "\n",
    "df = df[df[\"local_datetime_end\"] > df[\"local_datetime_start\"]]\n",
    "\n",
    "df = df[df[\"geohash7_orig\"].notna() & df[\"geohash7_dest\"].notna()]\n",
    "df[\"geohash7_orig\"] = df[\"geohash7_orig\"].astype(str)\n",
    "df[\"geohash7_dest\"] = df[\"geohash7_dest\"].astype(str)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2️⃣ FAST GEOHASH DECODE (OPTIMIZED: ONLY FIRST/last needed)\n",
    "# =========================\n",
    "\n",
    "def _decode_unique_geohash(series):\n",
    "    uniq = series.unique()\n",
    "    mapping = {}\n",
    "    for gh in uniq:\n",
    "        try:\n",
    "            lat, lon = pgh.decode(gh)\n",
    "            mapping[gh] = (lat, lon)\n",
    "        except Exception:\n",
    "            mapping[gh] = (np.nan, np.nan)\n",
    "    return series.map(mapping).apply(pd.Series).rename(columns={0: \"lat\", 1: \"lon\"})\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# OPT 1: avoid full-table geohash decode & avoid full sort\n",
    "# We first compute first/last row indices per linked_trip_id\n",
    "# using vectorized idxmin/idxmax on timestamps\n",
    "# -------------------------\n",
    "\n",
    "# drop rows with invalid times early (helps groupby idx ops)\n",
    "df = df[df[\"local_datetime_start\"].notna() & df[\"local_datetime_end\"].notna()]\n",
    "\n",
    "idx_first = df.groupby(\"linked_trip_id\")[\"local_datetime_start\"].idxmin()\n",
    "idx_last  = df.groupby(\"linked_trip_id\")[\"local_datetime_end\"].idxmax()\n",
    "\n",
    "first_trips = df.loc[idx_first, [\"linked_trip_id\", \"geohash7_orig\"]].copy()\n",
    "last_trips  = df.loc[idx_last,  [\"linked_trip_id\", \"geohash7_dest\"]].copy()\n",
    "\n",
    "# decode only needed geohashes (huge speedup)\n",
    "o_latlon = _decode_unique_geohash(first_trips[\"geohash7_orig\"])\n",
    "d_latlon = _decode_unique_geohash(last_trips[\"geohash7_dest\"])\n",
    "\n",
    "first_trips[\"orig_lat\"] = o_latlon[\"lat\"]\n",
    "first_trips[\"orig_lon\"] = o_latlon[\"lon\"]\n",
    "last_trips[\"dest_lat\"]  = d_latlon[\"lat\"]\n",
    "last_trips[\"dest_lon\"]  = d_latlon[\"lon\"]\n",
    "\n",
    "# drop decode failures before sjoin\n",
    "first_trips = first_trips[first_trips[\"orig_lat\"].notna() & first_trips[\"orig_lon\"].notna()]\n",
    "last_trips  = last_trips[last_trips[\"dest_lat\"].notna() & last_trips[\"dest_lon\"].notna()]\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3️⃣ TRIP-LEVEL OD FILTER (CORRECT, OPTIMIZED)\n",
    "# =========================\n",
    "\n",
    "# origin check (first trip only)\n",
    "gdf_o = gpd.GeoDataFrame(\n",
    "    first_trips,\n",
    "    geometry=gpd.points_from_xy(first_trips[\"orig_lon\"], first_trips[\"orig_lat\"]),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "gdf_o = gpd.sjoin(gdf_o, orig_poly, predicate=\"within\", how=\"inner\")\n",
    "valid_o_ids = set(gdf_o[\"linked_trip_id\"])\n",
    "\n",
    "# destination check (last trip only)\n",
    "gdf_d = gpd.GeoDataFrame(\n",
    "    last_trips,\n",
    "    geometry=gpd.points_from_xy(last_trips[\"dest_lon\"], last_trips[\"dest_lat\"]),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "gdf_d = gpd.sjoin(gdf_d, dest_poly, predicate=\"within\", how=\"inner\")\n",
    "valid_d_ids = set(gdf_d[\"linked_trip_id\"])\n",
    "\n",
    "valid_linked_ids = valid_o_ids & valid_d_ids\n",
    "\n",
    "# filter full df to keep all segments (same logic), then compute coords only for filtered data\n",
    "gdf = df[df[\"linked_trip_id\"].isin(valid_linked_ids)].copy()\n",
    "\n",
    "print(\"After OD (trip-level) filter, rows:\", len(gdf))\n",
    "print(\"Unique linked trips:\", gdf[\"linked_trip_id\"].nunique())\n",
    "\n",
    "if gdf.empty:\n",
    "    stats = {\n",
    "        \"schema\": \"nova.complete_trip.od_stats.v1\",\n",
    "        \"generated_at\": datetime.utcnow().isoformat() + \"Z\",\n",
    "        \"od\": {\"origin\": ORIG_TRACT, \"destination\": DEST_TRACT},\n",
    "        \"coverage\": {\"temporal\": \"year-2020\", \"spatial\": \"Salt Lake 6-county\"},\n",
    "        \"counts\": {\"linked_trips\": 0},\n",
    "        \"note\": \"No linked trips after trip-level OD filter\"\n",
    "    }\n",
    "    with open(OUTPUT_STATS_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(stats, f, indent=2, allow_nan=False)\n",
    "    raise SystemExit\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 4️⃣ (UNCHANGED LOGIC) OPTIONAL: decode all points for filtered data only\n",
    "# (kept to preserve your existing columns if you later reuse them)\n",
    "# =========================\n",
    "\n",
    "o_latlon_all = _decode_unique_geohash(gdf[\"geohash7_orig\"])\n",
    "d_latlon_all = _decode_unique_geohash(gdf[\"geohash7_dest\"])\n",
    "\n",
    "gdf[\"orig_lat\"] = o_latlon_all[\"lat\"]\n",
    "gdf[\"orig_lon\"] = o_latlon_all[\"lon\"]\n",
    "gdf[\"dest_lat\"] = d_latlon_all[\"lat\"]\n",
    "gdf[\"dest_lon\"] = d_latlon_all[\"lon\"]\n",
    "\n",
    "gdf = gdf[\n",
    "    gdf[\"orig_lat\"].notna() & gdf[\"orig_lon\"].notna() &\n",
    "    gdf[\"dest_lat\"].notna() & gdf[\"dest_lon\"].notna()\n",
    "]\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 5️⃣ DURATION\n",
    "# =========================\n",
    "\n",
    "gdf[\"duration_min\"] = (\n",
    "    gdf[\"local_datetime_end\"] - gdf[\"local_datetime_start\"]\n",
    ").dt.total_seconds() / 60\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 6️⃣ AGGREGATION\n",
    "# =========================\n",
    "\n",
    "trip_stats = (\n",
    "    gdf\n",
    "    .groupby(\"linked_trip_id\")\n",
    "    .agg(\n",
    "        total_duration=(\"duration_min\", \"sum\"),\n",
    "        segments=(\"duration_min\", \"size\"),\n",
    "        modes=(\"travel_mode\", lambda x: set(m.lower().strip() for m in x))\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 7️⃣ FINAL STATS\n",
    "# =========================\n",
    "\n",
    "dur = trip_stats[\"total_duration\"].to_numpy()\n",
    "segments = trip_stats[\"segments\"].to_numpy()\n",
    "modes = trip_stats[\"modes\"].to_numpy()\n",
    "\n",
    "def pct(a, q):\n",
    "    return float(np.percentile(a, q))\n",
    "\n",
    "BIN_WIDTH = 5\n",
    "MAX_TIME = 180\n",
    "\n",
    "bins = np.arange(0, MAX_TIME + BIN_WIDTH, BIN_WIDTH)\n",
    "dur_capped = np.clip(dur, 0, MAX_TIME)\n",
    "hist_counts, bin_edges = np.histogram(dur_capped, bins=bins)\n",
    "\n",
    "travel_time_hist = {\n",
    "    \"bin_width_min\": BIN_WIDTH,\n",
    "    \"max_time_min\": MAX_TIME,\n",
    "    \"bin_edges_min\": bin_edges.tolist(),\n",
    "    \"counts\": hist_counts.tolist()\n",
    "}\n",
    "\n",
    "stats = {\n",
    "    \"schema\": \"nova.complete_trip.od_stats.v1\",\n",
    "    \"generated_at\": datetime.utcnow().isoformat() + \"Z\",\n",
    "    \"od\": {\"origin\": ORIG_TRACT, \"destination\": DEST_TRACT},\n",
    "    \"coverage\": {\"temporal\": \"year-2020\", \"spatial\": \"Salt Lake 6-county\"},\n",
    "    \"counts\": {\"linked_trips\": int(len(dur))},\n",
    "    \"trip_duration_min\": {\n",
    "        \"min\": float(dur.min()),\n",
    "        \"mean\": float(dur.mean()),\n",
    "        \"p25\": pct(dur, 25),\n",
    "        \"median\": pct(dur, 50),\n",
    "        \"p75\": pct(dur, 75),\n",
    "        \"max\": float(dur.max())\n",
    "    },\n",
    "    \"segments\": {\n",
    "        \"avg\": float(segments.mean()),\n",
    "        \"p75\": int(pct(segments, 75)),\n",
    "        \"max\": int(segments.max())\n",
    "    },\n",
    "    \"mode_involvement\": {\n",
    "        \"car\": float(sum(\"car\" in m for m in modes) / len(modes)),\n",
    "        \"bus\": float(sum(\"bus\" in m for m in modes) / len(modes)),\n",
    "        \"rail\": float(sum(\"rail\" in m for m in modes) / len(modes)),\n",
    "        \"walk\": float(sum(\"walk\" in m for m in modes) / len(modes))\n",
    "    },\n",
    "    \"travel_time_distribution\": travel_time_hist\n",
    "}\n",
    "\n",
    "with open(OUTPUT_STATS_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(stats, f, indent=2, allow_nan=False)\n",
    "\n",
    "print(\"Stats JSON written to:\", OUTPUT_STATS_JSON)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63a3f26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 283 2034  987  229   76   24   13   11    5    4    7    4    3    3\n",
      "    1    2    1   12]\n"
     ]
    }
   ],
   "source": [
    "print(hist_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9df910",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
