{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58e810c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 49035114000 (center)\n",
    "# 49035980000 (airport)\n",
    "# 49035110106 (ski)\n",
    "# 49035101402 (U of U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "879c3751",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 126\u001b[39m\n\u001b[32m    123\u001b[39m last_trips  = df.loc[idx_last,  [\u001b[33m\"\u001b[39m\u001b[33mlinked_trip_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mgeohash7_dest\u001b[39m\u001b[33m\"\u001b[39m]].copy()\n\u001b[32m    125\u001b[39m o_latlon = _decode_unique_geohash(first_trips[\u001b[33m\"\u001b[39m\u001b[33mgeohash7_orig\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m d_latlon = \u001b[43m_decode_unique_geohash\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlast_trips\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgeohash7_dest\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m first_trips[\u001b[33m\"\u001b[39m\u001b[33morig_lat\u001b[39m\u001b[33m\"\u001b[39m] = o_latlon[\u001b[33m\"\u001b[39m\u001b[33mlat\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    129\u001b[39m first_trips[\u001b[33m\"\u001b[39m\u001b[33morig_lon\u001b[39m\u001b[33m\"\u001b[39m] = o_latlon[\u001b[33m\"\u001b[39m\u001b[33mlon\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 112\u001b[39m, in \u001b[36m_decode_unique_geohash\u001b[39m\u001b[34m(series)\u001b[39m\n\u001b[32m    110\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    111\u001b[39m         mapping[gh] = (np.nan, np.nan)\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mseries\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSeries\u001b[49m\u001b[43m)\u001b[49m.rename(columns={\u001b[32m0\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mlat\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mlon\u001b[39m\u001b[33m\"\u001b[39m})\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rli04\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4789\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4790\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4791\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4796\u001b[39m     **kwargs,\n\u001b[32m   4797\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4798\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4799\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4800\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4915\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4916\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4917\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4918\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4919\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4920\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4921\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4922\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4923\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4924\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rli04\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1424\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1426\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1427\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rli04\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1514\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1507\u001b[39m mapped = obj._map_values(\n\u001b[32m   1508\u001b[39m     mapper=curried, na_action=action, convert=\u001b[38;5;28mself\u001b[39m.convert_dtype\n\u001b[32m   1509\u001b[39m )\n\u001b[32m   1511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1512\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1513\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1514\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_constructor_expanddim\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmapped\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1515\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1516\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor(mapped, index=obj.index).__finalize__(\n\u001b[32m   1517\u001b[39m         obj, method=\u001b[33m\"\u001b[39m\u001b[33mapply\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1518\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rli04\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:851\u001b[39m, in \u001b[36mDataFrame.__init__\u001b[39m\u001b[34m(self, data, index, columns, dtype, copy)\u001b[39m\n\u001b[32m    849\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    850\u001b[39m         columns = ensure_index(columns)\n\u001b[32m--> \u001b[39m\u001b[32m851\u001b[39m     arrays, columns, index = \u001b[43mnested_data_to_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    852\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;49;00m\n\u001b[32m    853\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;49;00m\n\u001b[32m    854\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    855\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    856\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m    857\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    858\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    859\u001b[39m     mgr = arrays_to_mgr(\n\u001b[32m    860\u001b[39m         arrays,\n\u001b[32m    861\u001b[39m         columns,\n\u001b[32m   (...)\u001b[39m\u001b[32m    864\u001b[39m         typ=manager,\n\u001b[32m    865\u001b[39m     )\n\u001b[32m    866\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rli04\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:520\u001b[39m, in \u001b[36mnested_data_to_arrays\u001b[39m\u001b[34m(data, columns, index, dtype)\u001b[39m\n\u001b[32m    517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_named_tuple(data[\u001b[32m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    518\u001b[39m     columns = ensure_index(data[\u001b[32m0\u001b[39m]._fields)\n\u001b[32m--> \u001b[39m\u001b[32m520\u001b[39m arrays, columns = \u001b[43mto_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    521\u001b[39m columns = ensure_index(columns)\n\u001b[32m    523\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rli04\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:839\u001b[39m, in \u001b[36mto_arrays\u001b[39m\u001b[34m(data, columns, dtype)\u001b[39m\n\u001b[32m    837\u001b[39m     arr, columns = _list_of_dict_to_arrays(data, columns)\n\u001b[32m    838\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m--> \u001b[39m\u001b[32m839\u001b[39m     arr, columns = \u001b[43m_list_of_series_to_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    841\u001b[39m     \u001b[38;5;66;03m# last ditch effort\u001b[39;00m\n\u001b[32m    842\u001b[39m     data = [\u001b[38;5;28mtuple\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rli04\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:885\u001b[39m, in \u001b[36m_list_of_series_to_arrays\u001b[39m\u001b[34m(data, columns)\u001b[39m\n\u001b[32m    882\u001b[39m         indexer = indexer_cache[\u001b[38;5;28mid\u001b[39m(index)] = index.get_indexer(columns)\n\u001b[32m    884\u001b[39m     values = extract_array(s, extract_numpy=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m885\u001b[39m     aligned_values.append(\u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    887\u001b[39m content = np.vstack(aligned_values)\n\u001b[32m    888\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m content, columns\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rli04\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:117\u001b[39m, in \u001b[36mtake_nd\u001b[39m\u001b[34m(arr, indexer, axis, fill_value, allow_fill)\u001b[39m\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.take(indexer, fill_value=fill_value, allow_fill=allow_fill)\n\u001b[32m    116\u001b[39m arr = np.asarray(arr)\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_take_nd_ndarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rli04\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:162\u001b[39m, in \u001b[36m_take_nd_ndarray\u001b[39m\u001b[34m(arr, indexer, axis, fill_value, allow_fill)\u001b[39m\n\u001b[32m    157\u001b[39m     out = np.empty(out_shape, dtype=dtype)\n\u001b[32m    159\u001b[39m func = _get_take_nd_function(\n\u001b[32m    160\u001b[39m     arr.ndim, arr.dtype, out.dtype, axis=axis, mask_info=mask_info\n\u001b[32m    161\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m flip_order:\n\u001b[32m    165\u001b[39m     out = out.T\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# FAST + ROBUST VERSION\n",
    "# Build OD-level annual statistics for Complete Trips\n",
    "# (Statistics STRICTLY identical to legacy version)\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import glob\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pygeohash as pgh\n",
    "\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "\n",
    "BASE_DIR = \"C:/Users/rli04/Villanova University/Complete-trip-coordinate - Documents/General\"\n",
    "PARQUET_DIR = f\"{BASE_DIR}/Salt_Lake/delivery\"\n",
    "\n",
    "TRACT_SHP = (\n",
    "    f\"{BASE_DIR}/Manuscript/Figure/Visualization-RL/\"\n",
    "    f\"2-OD patterns by census track/six_counties_track.shp\"\n",
    ")\n",
    "\n",
    "TRACT_IDS = [\n",
    "    \"49035114000\",\n",
    "    \"49035980000\",\n",
    "    \"49035110106\",\n",
    "    \"49035101402\"\n",
    "]\n",
    "\n",
    "OD_PAIRS = [(o, d) for o in TRACT_IDS for d in TRACT_IDS if o != d]\n",
    "\n",
    "MONTHS = [\"Jan\"]\n",
    "\n",
    "USE_COLS = [\n",
    "    \"linked_trip_id\",\n",
    "    \"travel_mode\",\n",
    "    \"local_datetime_start\",\n",
    "    \"local_datetime_end\",\n",
    "    \"geohash7_orig\",\n",
    "    \"geohash7_dest\"\n",
    "]\n",
    "\n",
    "MAX_DIST_MILES = 1.0\n",
    "\n",
    "\n",
    "# =========================\n",
    "# GEO HELPERS (distance filter only)\n",
    "# =========================\n",
    "\n",
    "def haversine_miles(lon1, lat1, lon2, lat2):\n",
    "    R = 3958.8\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = (\n",
    "        np.sin(dlat / 2) ** 2\n",
    "        + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2) ** 2\n",
    "    )\n",
    "    return 2 * R * np.arcsin(np.sqrt(a))\n",
    "\n",
    "\n",
    "# =========================\n",
    "# LOAD TRACTS\n",
    "# =========================\n",
    "\n",
    "tracts = gpd.read_file(TRACT_SHP).to_crs(\"EPSG:4326\")\n",
    "tracts[\"GEOID\"] = tracts[\"GEOID\"].astype(str)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# LOAD PARQUET\n",
    "# =========================\n",
    "\n",
    "dfs = []\n",
    "for m in MONTHS:\n",
    "    files = glob.glob(f\"{PARQUET_DIR}/Salt_Lake-{m}-2020/*.snappy.parquet\")\n",
    "    dfs.append(pd.concat(\n",
    "        [pd.read_parquet(f, columns=USE_COLS) for f in files],\n",
    "        ignore_index=True\n",
    "    ))\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "df[\"local_datetime_start\"] = pd.to_datetime(df[\"local_datetime_start\"], errors=\"coerce\")\n",
    "df[\"local_datetime_end\"]   = pd.to_datetime(df[\"local_datetime_end\"], errors=\"coerce\")\n",
    "\n",
    "df = df[\n",
    "    (df[\"local_datetime_end\"] > df[\"local_datetime_start\"]) &\n",
    "    df[\"geohash7_orig\"].notna() &\n",
    "    df[\"geohash7_dest\"].notna()\n",
    "]\n",
    "\n",
    "\n",
    "# =========================\n",
    "# GEOHASH DECODE (helper)\n",
    "# =========================\n",
    "\n",
    "def _decode_unique_geohash(series):\n",
    "    uniq = series.unique()\n",
    "    mapping = {}\n",
    "    for gh in uniq:\n",
    "        try:\n",
    "            lat, lon = pgh.decode(gh)\n",
    "            mapping[gh] = (lat, lon)\n",
    "        except Exception:\n",
    "            mapping[gh] = (np.nan, np.nan)\n",
    "    return series.map(mapping).apply(pd.Series).rename(columns={0: \"lat\", 1: \"lon\"})\n",
    "\n",
    "\n",
    "# =========================\n",
    "# PRECOMPUTE FIRST / LAST\n",
    "# =========================\n",
    "\n",
    "idx_first = df.groupby(\"linked_trip_id\")[\"local_datetime_start\"].idxmin()\n",
    "idx_last  = df.groupby(\"linked_trip_id\")[\"local_datetime_end\"].idxmax()\n",
    "\n",
    "first_trips = df.loc[idx_first, [\"linked_trip_id\", \"geohash7_orig\"]].copy()\n",
    "last_trips  = df.loc[idx_last,  [\"linked_trip_id\", \"geohash7_dest\"]].copy()\n",
    "\n",
    "o_latlon = _decode_unique_geohash(first_trips[\"geohash7_orig\"])\n",
    "d_latlon = _decode_unique_geohash(last_trips[\"geohash7_dest\"])\n",
    "\n",
    "first_trips[\"orig_lat\"] = o_latlon[\"lat\"]\n",
    "first_trips[\"orig_lon\"] = o_latlon[\"lon\"]\n",
    "last_trips[\"dest_lat\"]  = d_latlon[\"lat\"]\n",
    "last_trips[\"dest_lon\"]  = d_latlon[\"lon\"]\n",
    "\n",
    "first_trips = first_trips.dropna(subset=[\"orig_lat\", \"orig_lon\"])\n",
    "last_trips  = last_trips.dropna(subset=[\"dest_lat\", \"dest_lon\"])\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN LOOP OVER OD PAIRS\n",
    "# ============================================================\n",
    "\n",
    "for ORIG_TRACT, DEST_TRACT in OD_PAIRS:\n",
    "\n",
    "    print(f\"\\n▶ Processing {ORIG_TRACT} → {DEST_TRACT}\")\n",
    "    OUTPUT_STATS_JSON = f\"{ORIG_TRACT}_to_{DEST_TRACT}.stats.json\"\n",
    "\n",
    "    orig_poly = tracts.loc[tracts[\"GEOID\"] == ORIG_TRACT, [\"geometry\"]]\n",
    "    dest_poly = tracts.loc[tracts[\"GEOID\"] == DEST_TRACT, [\"geometry\"]]\n",
    "\n",
    "    # ---- OD FILTER (OLD LOGIC) ----\n",
    "    gdf_o = gpd.GeoDataFrame(\n",
    "        first_trips,\n",
    "        geometry=gpd.points_from_xy(first_trips[\"orig_lon\"], first_trips[\"orig_lat\"]),\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "    valid_o_ids = set(gpd.sjoin(gdf_o, orig_poly, predicate=\"within\")[\"linked_trip_id\"])\n",
    "\n",
    "    gdf_d = gpd.GeoDataFrame(\n",
    "        last_trips,\n",
    "        geometry=gpd.points_from_xy(last_trips[\"dest_lon\"], last_trips[\"dest_lat\"]),\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "    valid_d_ids = set(gpd.sjoin(gdf_d, dest_poly, predicate=\"within\")[\"linked_trip_id\"])\n",
    "\n",
    "    valid_linked_ids = valid_o_ids & valid_d_ids\n",
    "\n",
    "    # ---- DISTANCE FILTER (ONLY HERE) ----\n",
    "    dist_ok_ids = set()\n",
    "    for _, r in first_trips[first_trips[\"linked_trip_id\"].isin(valid_linked_ids)].iterrows():\n",
    "        dist = haversine_miles(\n",
    "            r[\"orig_lon\"], r[\"orig_lat\"],\n",
    "            r[\"orig_lon\"], r[\"orig_lat\"]\n",
    "        )\n",
    "        if dist <= MAX_DIST_MILES:\n",
    "            dist_ok_ids.add(r[\"linked_trip_id\"])\n",
    "\n",
    "    valid_linked_ids = valid_linked_ids & dist_ok_ids\n",
    "\n",
    "\n",
    "    # ========================================================\n",
    "    # === OLD VERSION (UNCHANGED) FROM HERE ==================\n",
    "    # ========================================================\n",
    "\n",
    "    gdf = df[df[\"linked_trip_id\"].isin(valid_linked_ids)].copy()\n",
    "\n",
    "    if gdf.empty:\n",
    "        stats = {\n",
    "            \"schema\": \"nova.complete_trip.od_stats.v1\",\n",
    "            \"generated_at\": datetime.utcnow().isoformat() + \"Z\",\n",
    "            \"od\": {\"origin\": ORIG_TRACT, \"destination\": DEST_TRACT},\n",
    "            \"coverage\": {\"temporal\": \"year-2020\", \"spatial\": \"Salt Lake 6-county\"},\n",
    "            \"counts\": {\"linked_trips\": 0},\n",
    "            \"note\": \"No linked trips after OD + distance filter\"\n",
    "        }\n",
    "        with open(OUTPUT_STATS_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(stats, f, indent=2, allow_nan=False)\n",
    "        continue\n",
    "\n",
    "    # =========================\n",
    "    # 4️⃣ (UNCHANGED LOGIC)\n",
    "    # =========================\n",
    "\n",
    "    o_latlon_all = _decode_unique_geohash(gdf[\"geohash7_orig\"])\n",
    "    d_latlon_all = _decode_unique_geohash(gdf[\"geohash7_dest\"])\n",
    "\n",
    "    gdf[\"orig_lat\"] = o_latlon_all[\"lat\"]\n",
    "    gdf[\"orig_lon\"] = o_latlon_all[\"lon\"]\n",
    "    gdf[\"dest_lat\"] = d_latlon_all[\"lat\"]\n",
    "    gdf[\"dest_lon\"] = d_latlon_all[\"lon\"]\n",
    "\n",
    "    gdf = gdf[\n",
    "        gdf[\"orig_lat\"].notna() & gdf[\"orig_lon\"].notna() &\n",
    "        gdf[\"dest_lat\"].notna() & gdf[\"dest_lon\"].notna()\n",
    "    ]\n",
    "\n",
    "    # =========================\n",
    "    # 5️⃣ DURATION\n",
    "    # =========================\n",
    "\n",
    "    gdf[\"duration_min\"] = (\n",
    "        gdf[\"local_datetime_end\"] - gdf[\"local_datetime_start\"]\n",
    "    ).dt.total_seconds() / 60\n",
    "\n",
    "    # =========================\n",
    "    # 6️⃣ AGGREGATION\n",
    "    # =========================\n",
    "\n",
    "    trip_stats = (\n",
    "        gdf\n",
    "        .groupby(\"linked_trip_id\")\n",
    "        .agg(\n",
    "            total_duration=(\"duration_min\", \"sum\"),\n",
    "            segments=(\"duration_min\", \"size\"),\n",
    "            modes=(\"travel_mode\", lambda x: set(m.lower().strip() for m in x))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # =========================\n",
    "    # 7️⃣ FINAL STATS\n",
    "    # =========================\n",
    "\n",
    "    dur = trip_stats[\"total_duration\"].to_numpy()\n",
    "    segments = trip_stats[\"segments\"].to_numpy()\n",
    "    modes = trip_stats[\"modes\"].to_numpy()\n",
    "\n",
    "    def pct(a, q): return float(np.percentile(a, q))\n",
    "\n",
    "    BIN_WIDTH = 5\n",
    "    MAX_TIME = 180\n",
    "\n",
    "    bins = np.arange(0, MAX_TIME + BIN_WIDTH, BIN_WIDTH)\n",
    "    dur_capped = np.clip(dur, 0, MAX_TIME)\n",
    "    hist_counts, bin_edges = np.histogram(dur_capped, bins=bins)\n",
    "\n",
    "    travel_time_hist = {\n",
    "        \"bin_width_min\": BIN_WIDTH,\n",
    "        \"max_time_min\": MAX_TIME,\n",
    "        \"bin_edges_min\": bin_edges.tolist(),\n",
    "        \"counts\": hist_counts.tolist()\n",
    "    }\n",
    "\n",
    "    stats = {\n",
    "        \"schema\": \"nova.complete_trip.od_stats.v1\",\n",
    "        \"generated_at\": datetime.utcnow().isoformat() + \"Z\",\n",
    "        \"od\": {\"origin\": ORIG_TRACT, \"destination\": DEST_TRACT},\n",
    "        \"coverage\": {\"temporal\": \"year-2020\", \"spatial\": \"Salt Lake 6-county\"},\n",
    "        \"counts\": {\"linked_trips\": int(len(dur))},\n",
    "        \"trip_duration_min\": {\n",
    "            \"min\": float(dur.min()),\n",
    "            \"mean\": float(dur.mean()),\n",
    "            \"p25\": pct(dur, 25),\n",
    "            \"median\": pct(dur, 50),\n",
    "            \"p75\": pct(dur, 75),\n",
    "            \"max\": float(dur.max())\n",
    "        },\n",
    "        \"segments\": {\n",
    "            \"avg\": float(segments.mean()),\n",
    "            \"p75\": int(pct(segments, 75)),\n",
    "            \"max\": int(segments.max())\n",
    "        },\n",
    "        \"mode_involvement\": {\n",
    "            \"car\": float(sum(\"car\" in m for m in modes) / len(modes)),\n",
    "            \"bus\": float(sum(\"bus\" in m for m in modes) / len(modes)),\n",
    "            \"rail\": float(sum(\"rail\" in m for m in modes) / len(modes)),\n",
    "            \"walk\": float(sum(\"walk/bike\" in m for m in modes) / len(modes))\n",
    "        },\n",
    "        \"travel_time_distribution\": travel_time_hist\n",
    "    }\n",
    "\n",
    "    with open(OUTPUT_STATS_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(stats, f, indent=2, allow_nan=False)\n",
    "\n",
    "    print(\"✓ Stats JSON written:\", OUTPUT_STATS_JSON)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446a91a0",
   "metadata": {},
   "source": [
    "## Old version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a3f26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 283 2034  987  229   76   24   13   11    5    4    7    4    3    3\n",
      "    1    2    1   12]\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# FAST + ROBUST VERSION\n",
    "# Build OD-level annual statistics for Complete Trips\n",
    "# OD defined strictly by Census Tract (GEOID + shapefile)\n",
    "# Population-level statistics (NOT sampled)\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import glob\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pygeohash as pgh\n",
    "\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "\n",
    "BASE_DIR = \"C:/Users/rli04/Villanova University/Complete-trip-coordinate - Documents/General\"\n",
    "PARQUET_DIR = f\"{BASE_DIR}/Salt_Lake/delivery\"\n",
    "\n",
    "TRACT_SHP = (\n",
    "    f\"{BASE_DIR}/Manuscript/Figure/Visualization-RL/\"\n",
    "    f\"2-OD patterns by census track/six_counties_track.shp\"\n",
    ")\n",
    "\n",
    "ORIG_TRACT = \"49035101402\"\n",
    "DEST_TRACT = \"49035101402\"\n",
    "# 49035114000 (center)\n",
    "# 49035980000 (airport)\n",
    "# 49035110106 (ski)\n",
    "# 49035101402 (U of U)\n",
    "MONTHS = [\n",
    "    \"Jan\"\n",
    "    # ,\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\n",
    "    # \"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"\n",
    "]\n",
    "\n",
    "OUTPUT_STATS_JSON = f\"{ORIG_TRACT}_to_{DEST_TRACT}.stats.json\"\n",
    "\n",
    "USE_COLS = [\n",
    "    \"linked_trip_id\",\n",
    "    \"travel_mode\",\n",
    "    \"local_datetime_start\",\n",
    "    \"local_datetime_end\",\n",
    "    \"geohash7_orig\",\n",
    "    \"geohash7_dest\"\n",
    "]\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 0️⃣ LOAD TRACTS\n",
    "# =========================\n",
    "\n",
    "tracts = gpd.read_file(TRACT_SHP).to_crs(\"EPSG:4326\")\n",
    "tracts[\"GEOID\"] = tracts[\"GEOID\"].astype(str)\n",
    "\n",
    "orig_poly = tracts.loc[tracts[\"GEOID\"] == ORIG_TRACT, [\"geometry\"]].copy()\n",
    "dest_poly = tracts.loc[tracts[\"GEOID\"] == DEST_TRACT, [\"geometry\"]].copy()\n",
    "\n",
    "if orig_poly.empty or dest_poly.empty:\n",
    "    raise ValueError(\"Origin/Destination tract GEOID not found in shapefile.\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 1️⃣ LOAD YEARLY DATA\n",
    "# =========================\n",
    "\n",
    "dfs = []\n",
    "for m in MONTHS:\n",
    "    files = glob.glob(f\"{PARQUET_DIR}/Salt_Lake-{m}-2020/*.snappy.parquet\")\n",
    "    if not files:\n",
    "        continue\n",
    "    dfs.append(pd.concat(\n",
    "        [pd.read_parquet(f, columns=USE_COLS) for f in files],\n",
    "        ignore_index=True\n",
    "    ))\n",
    "\n",
    "if not dfs:\n",
    "    raise ValueError(\"No parquet files found. Check PARQUET_DIR and MONTHS.\")\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "df[\"local_datetime_start\"] = pd.to_datetime(df[\"local_datetime_start\"], errors=\"coerce\")\n",
    "df[\"local_datetime_end\"]   = pd.to_datetime(df[\"local_datetime_end\"], errors=\"coerce\")\n",
    "\n",
    "df = df[df[\"local_datetime_end\"] > df[\"local_datetime_start\"]]\n",
    "\n",
    "df = df[df[\"geohash7_orig\"].notna() & df[\"geohash7_dest\"].notna()]\n",
    "df[\"geohash7_orig\"] = df[\"geohash7_orig\"].astype(str)\n",
    "df[\"geohash7_dest\"] = df[\"geohash7_dest\"].astype(str)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2️⃣ FAST GEOHASH DECODE (OPTIMIZED: ONLY FIRST/last needed)\n",
    "# =========================\n",
    "\n",
    "def _decode_unique_geohash(series):\n",
    "    uniq = series.unique()\n",
    "    mapping = {}\n",
    "    for gh in uniq:\n",
    "        try:\n",
    "            lat, lon = pgh.decode(gh)\n",
    "            mapping[gh] = (lat, lon)\n",
    "        except Exception:\n",
    "            mapping[gh] = (np.nan, np.nan)\n",
    "    return series.map(mapping).apply(pd.Series).rename(columns={0: \"lat\", 1: \"lon\"})\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# OPT 1: avoid full-table geohash decode & avoid full sort\n",
    "# We first compute first/last row indices per linked_trip_id\n",
    "# using vectorized idxmin/idxmax on timestamps\n",
    "# -------------------------\n",
    "\n",
    "# drop rows with invalid times early (helps groupby idx ops)\n",
    "df = df[df[\"local_datetime_start\"].notna() & df[\"local_datetime_end\"].notna()]\n",
    "\n",
    "idx_first = df.groupby(\"linked_trip_id\")[\"local_datetime_start\"].idxmin()\n",
    "idx_last  = df.groupby(\"linked_trip_id\")[\"local_datetime_end\"].idxmax()\n",
    "\n",
    "first_trips = df.loc[idx_first, [\"linked_trip_id\", \"geohash7_orig\"]].copy()\n",
    "last_trips  = df.loc[idx_last,  [\"linked_trip_id\", \"geohash7_dest\"]].copy()\n",
    "\n",
    "# decode only needed geohashes (huge speedup)\n",
    "o_latlon = _decode_unique_geohash(first_trips[\"geohash7_orig\"])\n",
    "d_latlon = _decode_unique_geohash(last_trips[\"geohash7_dest\"])\n",
    "\n",
    "first_trips[\"orig_lat\"] = o_latlon[\"lat\"]\n",
    "first_trips[\"orig_lon\"] = o_latlon[\"lon\"]\n",
    "last_trips[\"dest_lat\"]  = d_latlon[\"lat\"]\n",
    "last_trips[\"dest_lon\"]  = d_latlon[\"lon\"]\n",
    "\n",
    "# drop decode failures before sjoin\n",
    "first_trips = first_trips[first_trips[\"orig_lat\"].notna() & first_trips[\"orig_lon\"].notna()]\n",
    "last_trips  = last_trips[last_trips[\"dest_lat\"].notna() & last_trips[\"dest_lon\"].notna()]\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3️⃣ TRIP-LEVEL OD FILTER (CORRECT, OPTIMIZED)\n",
    "# =========================\n",
    "\n",
    "# origin check (first trip only)\n",
    "gdf_o = gpd.GeoDataFrame(\n",
    "    first_trips,\n",
    "    geometry=gpd.points_from_xy(first_trips[\"orig_lon\"], first_trips[\"orig_lat\"]),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "gdf_o = gpd.sjoin(gdf_o, orig_poly, predicate=\"within\", how=\"inner\")\n",
    "valid_o_ids = set(gdf_o[\"linked_trip_id\"])\n",
    "\n",
    "# destination check (last trip only)\n",
    "gdf_d = gpd.GeoDataFrame(\n",
    "    last_trips,\n",
    "    geometry=gpd.points_from_xy(last_trips[\"dest_lon\"], last_trips[\"dest_lat\"]),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "gdf_d = gpd.sjoin(gdf_d, dest_poly, predicate=\"within\", how=\"inner\")\n",
    "valid_d_ids = set(gdf_d[\"linked_trip_id\"])\n",
    "\n",
    "valid_linked_ids = valid_o_ids & valid_d_ids\n",
    "\n",
    "# filter full df to keep all segments (same logic), then compute coords only for filtered data\n",
    "gdf = df[df[\"linked_trip_id\"].isin(valid_linked_ids)].copy()\n",
    "\n",
    "print(\"After OD (trip-level) filter, rows:\", len(gdf))\n",
    "print(\"Unique linked trips:\", gdf[\"linked_trip_id\"].nunique())\n",
    "\n",
    "if gdf.empty:\n",
    "    stats = {\n",
    "        \"schema\": \"nova.complete_trip.od_stats.v1\",\n",
    "        \"generated_at\": datetime.utcnow().isoformat() + \"Z\",\n",
    "        \"od\": {\"origin\": ORIG_TRACT, \"destination\": DEST_TRACT},\n",
    "        \"coverage\": {\"temporal\": \"year-2020\", \"spatial\": \"Salt Lake 6-county\"},\n",
    "        \"counts\": {\"linked_trips\": 0},\n",
    "        \"note\": \"No linked trips after trip-level OD filter\"\n",
    "    }\n",
    "    with open(OUTPUT_STATS_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(stats, f, indent=2, allow_nan=False)\n",
    "    raise SystemExit\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 4️⃣ (UNCHANGED LOGIC) OPTIONAL: decode all points for filtered data only\n",
    "# (kept to preserve your existing columns if you later reuse them)\n",
    "# =========================\n",
    "\n",
    "o_latlon_all = _decode_unique_geohash(gdf[\"geohash7_orig\"])\n",
    "d_latlon_all = _decode_unique_geohash(gdf[\"geohash7_dest\"])\n",
    "\n",
    "gdf[\"orig_lat\"] = o_latlon_all[\"lat\"]\n",
    "gdf[\"orig_lon\"] = o_latlon_all[\"lon\"]\n",
    "gdf[\"dest_lat\"] = d_latlon_all[\"lat\"]\n",
    "gdf[\"dest_lon\"] = d_latlon_all[\"lon\"]\n",
    "\n",
    "gdf = gdf[\n",
    "    gdf[\"orig_lat\"].notna() & gdf[\"orig_lon\"].notna() &\n",
    "    gdf[\"dest_lat\"].notna() & gdf[\"dest_lon\"].notna()\n",
    "]\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 5️⃣ DURATION\n",
    "# =========================\n",
    "\n",
    "gdf[\"duration_min\"] = (\n",
    "    gdf[\"local_datetime_end\"] - gdf[\"local_datetime_start\"]\n",
    ").dt.total_seconds() / 60\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 6️⃣ AGGREGATION\n",
    "# =========================\n",
    "\n",
    "trip_stats = (\n",
    "    gdf\n",
    "    .groupby(\"linked_trip_id\")\n",
    "    .agg(\n",
    "        total_duration=(\"duration_min\", \"sum\"),\n",
    "        segments=(\"duration_min\", \"size\"),\n",
    "        modes=(\"travel_mode\", lambda x: set(m.lower().strip() for m in x))\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 7️⃣ FINAL STATS\n",
    "# =========================\n",
    "\n",
    "dur = trip_stats[\"total_duration\"].to_numpy()\n",
    "segments = trip_stats[\"segments\"].to_numpy()\n",
    "modes = trip_stats[\"modes\"].to_numpy()\n",
    "\n",
    "def pct(a, q):\n",
    "    return float(np.percentile(a, q))\n",
    "\n",
    "BIN_WIDTH = 5\n",
    "MAX_TIME = 180\n",
    "\n",
    "bins = np.arange(0, MAX_TIME + BIN_WIDTH, BIN_WIDTH)\n",
    "dur_capped = np.clip(dur, 0, MAX_TIME)\n",
    "hist_counts, bin_edges = np.histogram(dur_capped, bins=bins)\n",
    "\n",
    "travel_time_hist = {\n",
    "    \"bin_width_min\": BIN_WIDTH,\n",
    "    \"max_time_min\": MAX_TIME,\n",
    "    \"bin_edges_min\": bin_edges.tolist(),\n",
    "    \"counts\": hist_counts.tolist()\n",
    "}\n",
    "\n",
    "stats = {\n",
    "    \"schema\": \"nova.complete_trip.od_stats.v1\",\n",
    "    \"generated_at\": datetime.utcnow().isoformat() + \"Z\",\n",
    "    \"od\": {\"origin\": ORIG_TRACT, \"destination\": DEST_TRACT},\n",
    "    \"coverage\": {\"temporal\": \"year-2020\", \"spatial\": \"Salt Lake 6-county\"},\n",
    "    \"counts\": {\"linked_trips\": int(len(dur))},\n",
    "    \"trip_duration_min\": {\n",
    "        \"min\": float(dur.min()),\n",
    "        \"mean\": float(dur.mean()),\n",
    "        \"p25\": pct(dur, 25),\n",
    "        \"median\": pct(dur, 50),\n",
    "        \"p75\": pct(dur, 75),\n",
    "        \"max\": float(dur.max())\n",
    "    },\n",
    "    \"segments\": {\n",
    "        \"avg\": float(segments.mean()),\n",
    "        \"p75\": int(pct(segments, 75)),\n",
    "        \"max\": int(segments.max())\n",
    "    },\n",
    "    \"mode_involvement\": {\n",
    "        \"car\": float(sum(\"car\" in m for m in modes) / len(modes)),\n",
    "        \"bus\": float(sum(\"bus\" in m for m in modes) / len(modes)),\n",
    "        \"rail\": float(sum(\"rail\" in m for m in modes) / len(modes)),\n",
    "        \"walk\": float(sum(\"walk/bike\" in m for m in modes) / len(modes))\n",
    "    },\n",
    "    \"travel_time_distribution\": travel_time_hist\n",
    "}\n",
    "\n",
    "with open(OUTPUT_STATS_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(stats, f, indent=2, allow_nan=False)\n",
    "\n",
    "print(\"Stats JSON written to:\", OUTPUT_STATS_JSON)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9df910",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
