{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58e810c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 49035114000 (center)\n",
    "# 49035980000 (airport)\n",
    "# 49035110106 (ski)\n",
    "# 49035101402 (U of U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "879c3751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After OD (tract-based) filter, rows: 217\n",
      "Unique linked trips: 217\n",
      "Stats JSON written to: 49035110106_to_49035101402.stats.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# FAST + ROBUST VERSION\n",
    "# Build OD-level annual statistics for Complete Trips\n",
    "# OD defined strictly by Census Tract (GEOID + shapefile)\n",
    "# Population-level statistics (NOT sampled)\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import glob\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pygeohash as pgh\n",
    "\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "\n",
    "BASE_DIR = \"C:/Users/rli04/Villanova University/Complete-trip-coordinate - Documents/General\"\n",
    "PARQUET_DIR = f\"{BASE_DIR}/Salt_Lake/delivery\"\n",
    "\n",
    "TRACT_SHP = (\n",
    "    f\"{BASE_DIR}/Manuscript/Figure/Visualization-RL/\"\n",
    "    f\"2-OD patterns by census track/six_counties_track.shp\"\n",
    ")\n",
    "\n",
    "ORIG_TRACT = \"49035110106\"\n",
    "DEST_TRACT = \"49035101402\"\n",
    "# 49035114000 (center)\n",
    "# 49035980000 (airport)\n",
    "# 49035110106 (ski)\n",
    "# 49035101402 (U of U)\n",
    "MONTHS = [\n",
    "    \"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\n",
    "    \"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"\n",
    "]\n",
    "\n",
    "OUTPUT_STATS_JSON = f\"{ORIG_TRACT}_to_{DEST_TRACT}.stats.json\"\n",
    "\n",
    "USE_COLS = [\n",
    "    \"linked_trip_id\",\n",
    "    \"travel_mode\",\n",
    "    \"local_datetime_start\",\n",
    "    \"local_datetime_end\",\n",
    "    \"geohash7_orig\",\n",
    "    \"geohash7_dest\"\n",
    "]\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 0️⃣ LOAD TRACTS\n",
    "# =========================\n",
    "\n",
    "tracts = gpd.read_file(TRACT_SHP).to_crs(\"EPSG:4326\")\n",
    "tracts[\"GEOID\"] = tracts[\"GEOID\"].astype(str)\n",
    "\n",
    "orig_poly = tracts.loc[tracts[\"GEOID\"] == ORIG_TRACT, [\"geometry\"]].copy()\n",
    "dest_poly = tracts.loc[tracts[\"GEOID\"] == DEST_TRACT, [\"geometry\"]].copy()\n",
    "\n",
    "if orig_poly.empty or dest_poly.empty:\n",
    "    raise ValueError(\"Origin/Destination tract GEOID not found in shapefile.\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 1️⃣ LOAD YEARLY DATA\n",
    "# =========================\n",
    "\n",
    "dfs = []\n",
    "for m in MONTHS:\n",
    "    files = glob.glob(f\"{PARQUET_DIR}/Salt_Lake-{m}-2020/*.snappy.parquet\")\n",
    "    if not files:\n",
    "        continue\n",
    "    dfs.append(pd.concat(\n",
    "        [pd.read_parquet(f, columns=USE_COLS) for f in files],\n",
    "        ignore_index=True\n",
    "    ))\n",
    "\n",
    "if not dfs:\n",
    "    raise ValueError(\"No parquet files found. Check PARQUET_DIR and MONTHS.\")\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "df[\"local_datetime_start\"] = pd.to_datetime(df[\"local_datetime_start\"], errors=\"coerce\")\n",
    "df[\"local_datetime_end\"]   = pd.to_datetime(df[\"local_datetime_end\"], errors=\"coerce\")\n",
    "\n",
    "df = df[df[\"local_datetime_end\"] > df[\"local_datetime_start\"]]\n",
    "\n",
    "# 清理 geohash 空值/非字符串（避免 pgh.decode 崩）\n",
    "df = df[df[\"geohash7_orig\"].notna() & df[\"geohash7_dest\"].notna()]\n",
    "df[\"geohash7_orig\"] = df[\"geohash7_orig\"].astype(str)\n",
    "df[\"geohash7_dest\"] = df[\"geohash7_dest\"].astype(str)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2️⃣ FAST GEOHASH DECODE (UNIQUE CACHE)\n",
    "# =========================\n",
    "\n",
    "def _decode_unique_geohash(series: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Decode geohash series into (lat, lon) using unique-cache mapping.\n",
    "    Invalid geohash -> NaN.\n",
    "    \"\"\"\n",
    "    uniq = series.unique()\n",
    "    mapping = {}\n",
    "    for gh in uniq:\n",
    "        try:\n",
    "            lat, lon = pgh.decode(gh)\n",
    "            mapping[gh] = (lat, lon)\n",
    "        except Exception:\n",
    "            mapping[gh] = (np.nan, np.nan)\n",
    "\n",
    "    latlon = series.map(mapping)\n",
    "    out = pd.DataFrame(latlon.tolist(), index=series.index, columns=[\"lat\", \"lon\"])\n",
    "    return out\n",
    "\n",
    "o_latlon = _decode_unique_geohash(df[\"geohash7_orig\"])\n",
    "d_latlon = _decode_unique_geohash(df[\"geohash7_dest\"])\n",
    "\n",
    "df[\"orig_lat\"] = o_latlon[\"lat\"]\n",
    "df[\"orig_lon\"] = o_latlon[\"lon\"]\n",
    "df[\"dest_lat\"] = d_latlon[\"lat\"]\n",
    "df[\"dest_lon\"] = d_latlon[\"lon\"]\n",
    "\n",
    "# 丢掉 decode 失败的点\n",
    "df = df[df[\"orig_lat\"].notna() & df[\"orig_lon\"].notna() & df[\"dest_lat\"].notna() & df[\"dest_lon\"].notna()]\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3️⃣ OD FILTER (SPATIAL JOIN, FAST)\n",
    "# =========================\n",
    "\n",
    "# Origin points GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    df,\n",
    "    geometry=gpd.points_from_xy(df[\"orig_lon\"], df[\"orig_lat\"]),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "# sjoin origin within ORIG tract\n",
    "gdf = gpd.sjoin(gdf, orig_poly, predicate=\"within\", how=\"inner\")\n",
    "# 清理 join 产生的列，避免第二次 sjoin 冲突\n",
    "for col in [\"index_right\"]:\n",
    "    if col in gdf.columns:\n",
    "        gdf = gdf.drop(columns=[col])\n",
    "\n",
    "# Destination points GeoDataFrame（替换 geometry 为 dest 点）\n",
    "gdf = gdf.drop(columns=[\"geometry\"])\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    gdf,\n",
    "    geometry=gpd.points_from_xy(gdf[\"dest_lon\"], gdf[\"dest_lat\"]),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "# sjoin destination within DEST tract\n",
    "gdf = gpd.sjoin(gdf, dest_poly, predicate=\"within\", how=\"inner\")\n",
    "for col in [\"index_right\"]:\n",
    "    if col in gdf.columns:\n",
    "        gdf = gdf.drop(columns=[col])\n",
    "\n",
    "print(\"After OD (tract-based) filter, rows:\", len(gdf))\n",
    "print(\"Unique linked trips:\", gdf[\"linked_trip_id\"].nunique())\n",
    "\n",
    "# 如果 OD 没有任何记录，直接输出空 stats（不报错）\n",
    "if gdf.empty:\n",
    "    stats = {\n",
    "        \"schema\": \"nova.complete_trip.od_stats.v1\",\n",
    "        \"generated_at\": datetime.utcnow().isoformat() + \"Z\",\n",
    "        \"od\": {\"origin\": ORIG_TRACT, \"destination\": DEST_TRACT},\n",
    "        \"coverage\": {\"temporal\": \"year-2020\", \"spatial\": \"Salt Lake 6-county\"},\n",
    "        \"counts\": {\"linked_trips\": 0},\n",
    "        \"note\": \"No rows after tract-based OD filter\"\n",
    "    }\n",
    "    with open(OUTPUT_STATS_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(stats, f, indent=2, allow_nan=False)\n",
    "    print(\"Stats JSON written to:\", OUTPUT_STATS_JSON)\n",
    "    raise SystemExit\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 4️⃣ COMPLETE TRIP FILTER (≥2 legs)  ——逻辑不变\n",
    "# =========================\n",
    "\n",
    "leg_counts = gdf.groupby(\"linked_trip_id\").size()\n",
    "\n",
    "\n",
    "if gdf.empty:\n",
    "    stats = {\n",
    "        \"schema\": \"nova.complete_trip.od_stats.v1\",\n",
    "        \"generated_at\": datetime.utcnow().isoformat() + \"Z\",\n",
    "        \"od\": {\"origin\": ORIG_TRACT, \"destination\": DEST_TRACT},\n",
    "        \"coverage\": {\"temporal\": \"year-2020\", \"spatial\": \"Salt Lake 6-county\"},\n",
    "        \"counts\": {\"linked_trips\": 0},\n",
    "        \"note\": \"No valid complete trips (linked_trip with >=2 legs) after filtering\"\n",
    "    }\n",
    "    with open(OUTPUT_STATS_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(stats, f, indent=2, allow_nan=False)\n",
    "    print(\"Stats JSON written to:\", OUTPUT_STATS_JSON)\n",
    "    raise SystemExit\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 5️⃣ LEG DURATION (VECTOR)\n",
    "# =========================\n",
    "\n",
    "gdf[\"duration_min\"] = (\n",
    "    gdf[\"local_datetime_end\"] - gdf[\"local_datetime_start\"]\n",
    ").dt.total_seconds() / 60\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 6️⃣ COMPLETE-TRIP AGGREGATION (FAST)\n",
    "# =========================\n",
    "\n",
    "trip_stats = (\n",
    "    gdf\n",
    "    .groupby(\"linked_trip_id\")\n",
    "    .agg(\n",
    "        total_duration=(\"duration_min\", \"sum\"),\n",
    "        transfers=(\"duration_min\", \"size\"),\n",
    "        modes=(\"travel_mode\", lambda x: set(m.lower().strip() for m in x))\n",
    "    )\n",
    ")\n",
    "\n",
    "trip_stats[\"transfers\"] = trip_stats[\"transfers\"] - 1\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 7️⃣ FINAL STATS (WITH EMPTY GUARD)\n",
    "# =========================\n",
    "\n",
    "dur = trip_stats[\"total_duration\"].to_numpy()\n",
    "mean_dur = float(dur.mean())\n",
    "trf = trip_stats[\"transfers\"].to_numpy()\n",
    "modes = trip_stats[\"modes\"].to_numpy()\n",
    "\n",
    "def pct(a, q):\n",
    "    return float(np.percentile(a, q))\n",
    "# =========================\n",
    "# 7️⃣b TRAVEL TIME DISTRIBUTION (HISTOGRAM)\n",
    "# =========================\n",
    "\n",
    "BIN_WIDTH = 5        # minutes\n",
    "MAX_TIME  = 180       # minutes (cap long tail)\n",
    "\n",
    "bins = np.arange(0, MAX_TIME + BIN_WIDTH, BIN_WIDTH)\n",
    "\n",
    "# cap extreme values\n",
    "dur_capped = np.clip(dur, 0, MAX_TIME)\n",
    "\n",
    "hist_counts, bin_edges = np.histogram(dur_capped, bins=bins)\n",
    "\n",
    "travel_time_hist = {\n",
    "    \"bin_width_min\": BIN_WIDTH,\n",
    "    \"max_time_min\": MAX_TIME,\n",
    "    \"bin_edges_min\": bin_edges.tolist(),\n",
    "    \"counts\": hist_counts.tolist()\n",
    "}\n",
    "\n",
    "\n",
    "stats = {\n",
    "    \"schema\": \"nova.complete_trip.od_stats.v1\",\n",
    "    \"generated_at\": datetime.utcnow().isoformat() + \"Z\",\n",
    "    \"od\": {\"origin\": ORIG_TRACT, \"destination\": DEST_TRACT},\n",
    "    \"coverage\": {\"temporal\": \"year-2020\", \"spatial\": \"Salt Lake 6-county\"},\n",
    "    \"counts\": {\"linked_trips\": int(len(dur))},\n",
    "    \"trip_duration_min\": {\n",
    "        \"min\": float(dur.min()),\n",
    "        \"mean\": mean_dur,\n",
    "        \"p25\": pct(dur, 25),\n",
    "        \"median\": pct(dur, 50),\n",
    "        \"p75\": pct(dur, 75),\n",
    "        \"max\": float(dur.max())\n",
    "    },\n",
    "    \"transfers\": {\n",
    "        \"avg\": float(trf.mean()),\n",
    "        \"p75\": int(pct(trf, 75)),\n",
    "        \"max\": int(trf.max())\n",
    "    },\n",
    "    \"mode_involvement\": {\n",
    "        \"car\": float(sum(\"car\" in m for m in modes) / len(modes)),\n",
    "        \"bus\": float(sum(\"bus\" in m for m in modes) / len(modes)),\n",
    "        \"rail\": float(sum(\"rail\" in m for m in modes) / len(modes)),\n",
    "        \"walk\": float(sum(\"walk\" in m for m in modes) / len(modes))\n",
    "    },\n",
    "    \"travel_time_distribution\": travel_time_hist\n",
    "\n",
    "}\n",
    "\n",
    "with open(OUTPUT_STATS_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(stats, f, indent=2, allow_nan=False)\n",
    "\n",
    "print(\"Stats JSON written to:\", OUTPUT_STATS_JSON)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63a3f26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 283 2034  987  229   76   24   13   11    5    4    7    4    3    3\n",
      "    1    2    1   12]\n"
     ]
    }
   ],
   "source": [
    "print(hist_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9df910",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
