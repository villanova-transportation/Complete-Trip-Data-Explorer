{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f1b71ad",
   "metadata": {},
   "source": [
    "Columns:\n",
    "['travel_mode', 'linked_trip_id', 'trip_id', 'tour_id', 'device_id', 'geohash7_orig', 'geohash7_dest', 'local_datetime_start', 'local_datetime_end', 'route_taken', 'observed_link', 'observed_time', 'network_distance', 'route_distance', 'route_speed', 'geohash7_home', 'access_stop', 'access_stop_id', 'egress_stop', 'egress_stop_id', 'trip_purpose', 'trip_weight', 'flag_enter', 'flag_exit']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f971e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/26 [02:46<1:09:28, 166.73s/it]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['GEOID'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 98\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m# Spatial join (origin)\u001b[39;00m\n\u001b[32m     90\u001b[39m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[32m     91\u001b[39m o_gdf = gpd.GeoDataFrame(\n\u001b[32m     92\u001b[39m     df,\n\u001b[32m     93\u001b[39m     geometry=gpd.points_from_xy(df.o_lon, df.o_lat),\n\u001b[32m     94\u001b[39m     crs=\u001b[33m\"\u001b[39m\u001b[33mEPSG:4326\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     95\u001b[39m )\n\u001b[32m     96\u001b[39m o_join = gpd.sjoin(\n\u001b[32m     97\u001b[39m     o_gdf,\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m     \u001b[43mtracts\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mTRACT_COL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgeometry\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m,\n\u001b[32m     99\u001b[39m     how=\u001b[33m\"\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    100\u001b[39m     predicate=\u001b[33m\"\u001b[39m\u001b[33mintersects\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    101\u001b[39m ).rename(columns={TRACT_COL: \u001b[33m\"\u001b[39m\u001b[33morigin_tract\u001b[39m\u001b[33m\"\u001b[39m})\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[38;5;66;03m# Spatial join (destination)\u001b[39;00m\n\u001b[32m    105\u001b[39m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[32m    106\u001b[39m d_gdf = gpd.GeoDataFrame(\n\u001b[32m    107\u001b[39m     o_join,\n\u001b[32m    108\u001b[39m     geometry=gpd.points_from_xy(o_join.d_lon, o_join.d_lat),\n\u001b[32m    109\u001b[39m     crs=\u001b[33m\"\u001b[39m\u001b[33mEPSG:4326\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    110\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rli04\\AppData\\Local\\anaconda3\\Lib\\site-packages\\geopandas\\geodataframe.py:1456\u001b[39m, in \u001b[36mGeoDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1450\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[32m   1451\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1452\u001b[39m \u001b[33;03m    If the result is a column containing only 'geometry', return a\u001b[39;00m\n\u001b[32m   1453\u001b[39m \u001b[33;03m    GeoSeries. If it's a DataFrame with any columns of GeometryDtype,\u001b[39;00m\n\u001b[32m   1454\u001b[39m \u001b[33;03m    return a GeoDataFrame.\u001b[39;00m\n\u001b[32m   1455\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1456\u001b[39m     result = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1457\u001b[39m     \u001b[38;5;66;03m# Custom logic to avoid waiting for pandas GH51895\u001b[39;00m\n\u001b[32m   1458\u001b[39m     \u001b[38;5;66;03m# result is not geometry dtype for multi-indexes\u001b[39;00m\n\u001b[32m   1459\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1460\u001b[39m         pd.api.types.is_scalar(key)\n\u001b[32m   1461\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m key == \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1464\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_geometry_type(result)\n\u001b[32m   1465\u001b[39m     ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rli04\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4107\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4108\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4110\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rli04\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6197\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6198\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6200\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6202\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6204\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rli04\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6249\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6251\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m-> \u001b[39m\u001b[32m6252\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"['GEOID'] not in index\""
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================\n",
    "# Paths\n",
    "# =========================\n",
    "DELIVERY_ROOT = r\"C:\\Users\\rli04\\Villanova University\\Complete-trip-coordinate - Documents\\General\\Salt_Lake\\delivery\"\n",
    "CENSUS_FILE = r\"C:\\Github\\Complete-Trip-Data-Explorer\\data\\census_track\\CensusTracts2020_6_counties.geojson\"\n",
    "OUT_JSON = r\"./od_monthly_linked_unlinked.json\"\n",
    "\n",
    "# =========================\n",
    "# Load census tracts\n",
    "# =========================\n",
    "tracts = gpd.read_file(CENSUS_FILE).to_crs(epsg=4326)\n",
    "TRACT_COL = \"GEOID\"\n",
    "\n",
    "# =========================\n",
    "# Pure python geohash decode\n",
    "# =========================\n",
    "_base32 = \"0123456789bcdefghjkmnpqrstuvwxyz\"\n",
    "_base32_map = {c: i for i, c in enumerate(_base32)}\n",
    "\n",
    "def decode_geohash(gh):\n",
    "    lat_interval = [-90.0, 90.0]\n",
    "    lon_interval = [-180.0, 180.0]\n",
    "    is_even = True\n",
    "    for c in gh:\n",
    "        cd = _base32_map[c]\n",
    "        for mask in (16, 8, 4, 2, 1):\n",
    "            if is_even:\n",
    "                mid = (lon_interval[0] + lon_interval[1]) / 2\n",
    "                lon_interval[0 if cd & mask else 1] = mid\n",
    "            else:\n",
    "                mid = (lat_interval[0] + lat_interval[1]) / 2\n",
    "                lat_interval[0 if cd & mask else 1] = mid\n",
    "            is_even = not is_even\n",
    "    return (lon_interval[0] + lon_interval[1]) / 2, (lat_interval[0] + lat_interval[1]) / 2\n",
    "\n",
    "# =========================\n",
    "# Main container\n",
    "# =========================\n",
    "all_months = []\n",
    "\n",
    "# =========================\n",
    "# Loop months\n",
    "# =========================\n",
    "for folder in tqdm(os.listdir(DELIVERY_ROOT)):\n",
    "    if not folder.startswith(\"Salt_Lake-\"):\n",
    "        continue\n",
    "\n",
    "    month_dir = os.path.join(DELIVERY_ROOT, folder)\n",
    "\n",
    "    dfs = [\n",
    "        pd.read_parquet(os.path.join(month_dir, f))\n",
    "        for f in os.listdir(month_dir)\n",
    "        if f.endswith(\".parquet\")\n",
    "    ]\n",
    "    if not dfs:\n",
    "        continue\n",
    "\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # =========================\n",
    "    # Basic cleaning\n",
    "    # =========================\n",
    "    df = df[\n",
    "        df[\"geohash7_orig\"].notna() &\n",
    "        df[\"geohash7_dest\"].notna()\n",
    "    ].copy()\n",
    "\n",
    "    df[\"local_datetime_start\"] = pd.to_datetime(df[\"local_datetime_start\"])\n",
    "    df[\"month\"] = df[\"local_datetime_start\"].dt.to_period(\"M\").astype(str)\n",
    "    df[\"trip_weight\"] = df[\"trip_weight\"].fillna(1.0)\n",
    "\n",
    "    # =========================\n",
    "    # Decode geohash → lon/lat\n",
    "    # =========================\n",
    "    df[[\"o_lon\", \"o_lat\"]] = df[\"geohash7_orig\"].apply(\n",
    "        lambda x: pd.Series(decode_geohash(x))\n",
    "    )\n",
    "    df[[\"d_lon\", \"d_lat\"]] = df[\"geohash7_dest\"].apply(\n",
    "        lambda x: pd.Series(decode_geohash(x))\n",
    "    )\n",
    "\n",
    "    # =========================\n",
    "    # Spatial join (origin)\n",
    "    # =========================\n",
    "    o_gdf = gpd.GeoDataFrame(\n",
    "        df,\n",
    "        geometry=gpd.points_from_xy(df.o_lon, df.o_lat),\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "    o_join = gpd.sjoin(\n",
    "        o_gdf,\n",
    "        tracts[[TRACT_COL, \"geometry\"]],\n",
    "        how=\"left\",\n",
    "        predicate=\"intersects\"\n",
    "    ).rename(columns={TRACT_COL: \"origin_tract\"})\n",
    "\n",
    "    # =========================\n",
    "    # Spatial join (destination)\n",
    "    # =========================\n",
    "    d_gdf = gpd.GeoDataFrame(\n",
    "        o_join,\n",
    "        geometry=gpd.points_from_xy(o_join.d_lon, o_join.d_lat),\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "    d_join = gpd.sjoin(\n",
    "        d_gdf,\n",
    "        tracts[[TRACT_COL, \"geometry\"]],\n",
    "        how=\"left\",\n",
    "        predicate=\"intersects\"\n",
    "    ).rename(columns={TRACT_COL: \"destination_tract\"})\n",
    "\n",
    "    # =========================\n",
    "    # -------- UNLINKED OD --------\n",
    "    # =========================\n",
    "    unlinked_od = (\n",
    "        d_join\n",
    "        .groupby(\n",
    "            [\n",
    "                \"month\",\n",
    "                \"origin_tract\",\n",
    "                \"destination_tract\",\n",
    "                \"travel_mode\",\n",
    "                \"o_lon\", \"o_lat\",\n",
    "                \"d_lon\", \"d_lat\"\n",
    "            ],\n",
    "            as_index=False\n",
    "        )\n",
    "        .agg(\n",
    "            unlinked_count=(\"trip_id\", \"count\"),\n",
    "            unlinked_weighted_flow=(\"trip_weight\", \"sum\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # =========================\n",
    "    # -------- LINKED OD (CORRECT) --------\n",
    "    # =========================\n",
    "    # 关键：先按时间排序，再 collapse\n",
    "    d_join = d_join.sort_values([\"linked_trip_id\", \"local_datetime_start\"])\n",
    "\n",
    "    linked_base = (\n",
    "        d_join\n",
    "        .groupby(\"linked_trip_id\", as_index=False)\n",
    "        .agg(\n",
    "            month=(\"month\", \"first\"),\n",
    "            travel_mode=(\"travel_mode\", \"first\"),\n",
    "\n",
    "            origin_tract=(\"origin_tract\", \"first\"),\n",
    "            o_lon=(\"o_lon\", \"first\"),\n",
    "            o_lat=(\"o_lat\", \"first\"),\n",
    "\n",
    "            destination_tract=(\"destination_tract\", \"last\"),\n",
    "            d_lon=(\"d_lon\", \"last\"),\n",
    "            d_lat=(\"d_lat\", \"last\"),\n",
    "\n",
    "            linked_weight=(\"trip_weight\", \"first\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    linked_od = (\n",
    "        linked_base\n",
    "        .groupby(\n",
    "            [\n",
    "                \"month\",\n",
    "                \"origin_tract\",\n",
    "                \"destination_tract\",\n",
    "                \"travel_mode\",\n",
    "                \"o_lon\", \"o_lat\",\n",
    "                \"d_lon\", \"d_lat\"\n",
    "            ],\n",
    "            as_index=False\n",
    "        )\n",
    "        .agg(\n",
    "            linked_count=(\"linked_trip_id\", \"count\"),\n",
    "            linked_weighted_flow=(\"linked_weight\", \"sum\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # =========================\n",
    "    # Merge linked + unlinked\n",
    "    # =========================\n",
    "    od = pd.merge(\n",
    "        unlinked_od,\n",
    "        linked_od,\n",
    "        on=[\n",
    "            \"month\",\n",
    "            \"origin_tract\",\n",
    "            \"destination_tract\",\n",
    "            \"travel_mode\",\n",
    "            \"o_lon\", \"o_lat\",\n",
    "            \"d_lon\", \"d_lat\"\n",
    "        ],\n",
    "        how=\"outer\"\n",
    "    )\n",
    "\n",
    "    all_months.append(od)\n",
    "\n",
    "# =========================\n",
    "# Final output\n",
    "# =========================\n",
    "final_df = pd.concat(all_months, ignore_index=True)\n",
    "\n",
    "# JSON safety\n",
    "final_df = final_df.fillna(0)\n",
    "final_df[[\"origin_tract\", \"destination_tract\"]] = (\n",
    "    final_df[[\"origin_tract\", \"destination_tract\"]].replace(0, \"UNKNOWN\")\n",
    ")\n",
    "\n",
    "with open(OUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(final_df.to_dict(orient=\"records\"), f, indent=2)\n",
    "\n",
    "print(f\"Saved {len(final_df)} OD records to {OUT_JSON}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03f45d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rli04\\AppData\\Local\\Temp\\ipykernel_13432\\487544621.py:26: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  tracts[\"centroid\"] = tracts.geometry.centroid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved tract centroids → ./tract_centroids.json\n",
      "Building geohash → tract lookup (ONE TIME)...\n",
      "Unique geohash count: 191678\n",
      "Geohash → tract lookup built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing months: 100%|██████████| 26/26 [12:10<00:00, 28.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved tract-level OD → ./od_monthly_linked_unlinked.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import pygeohash as pgh\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================\n",
    "# Paths\n",
    "# =========================\n",
    "DELIVERY_ROOT = r\"C:\\Users\\rli04\\Villanova University\\Complete-trip-coordinate - Documents\\General\\Salt_Lake\\delivery\"\n",
    "CENSUS_FILE = r\"C:\\Github\\Complete-Trip-Data-Explorer\\data\\census_track\\CensusTracts2020_6_counties.geojson\"\n",
    "\n",
    "OUT_OD_JSON = r\"./od_monthly_linked_unlinked.json\"\n",
    "OUT_TRACT_CENTROID_JSON = r\"./tract_centroids.json\"\n",
    "\n",
    "# =========================\n",
    "# Load census tracts\n",
    "# =========================\n",
    "tracts = gpd.read_file(CENSUS_FILE).to_crs(epsg=4326)\n",
    "TRACT_COL = \"GEOID20\"\n",
    "\n",
    "# =========================\n",
    "# Precompute tract centroids (ONCE)\n",
    "# =========================\n",
    "tracts[\"centroid\"] = tracts.geometry.centroid\n",
    "\n",
    "tract_centroids = {\n",
    "    str(row[TRACT_COL]): {\n",
    "        \"lat\": row.centroid.y,\n",
    "        \"lon\": row.centroid.x\n",
    "    }\n",
    "    for _, row in tracts.iterrows()\n",
    "}\n",
    "\n",
    "with open(OUT_TRACT_CENTROID_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(tract_centroids, f, indent=2)\n",
    "\n",
    "print(f\"Saved tract centroids → {OUT_TRACT_CENTROID_JSON}\")\n",
    "\n",
    "# =========================\n",
    "# Build GLOBAL geohash → tract lookup\n",
    "# =========================\n",
    "print(\"Building geohash → tract lookup (ONE TIME)...\")\n",
    "\n",
    "all_geohash = set()\n",
    "\n",
    "for folder in os.listdir(DELIVERY_ROOT):\n",
    "    if not folder.startswith(\"Salt_Lake-\"):\n",
    "        continue\n",
    "    month_dir = os.path.join(DELIVERY_ROOT, folder)\n",
    "    for f in os.listdir(month_dir):\n",
    "        if f.endswith(\".parquet\"):\n",
    "            tmp = pd.read_parquet(\n",
    "                os.path.join(month_dir, f),\n",
    "                columns=[\"geohash7_orig\", \"geohash7_dest\"]\n",
    "            )\n",
    "            all_geohash.update(tmp[\"geohash7_orig\"].dropna().unique())\n",
    "            all_geohash.update(tmp[\"geohash7_dest\"].dropna().unique())\n",
    "\n",
    "all_geohash = list(all_geohash)\n",
    "print(f\"Unique geohash count: {len(all_geohash)}\")\n",
    "\n",
    "# decode once\n",
    "gh_df = pd.DataFrame({\n",
    "    \"geohash\": all_geohash,\n",
    "    \"lat\": [pgh.decode(g)[0] for g in all_geohash],\n",
    "    \"lon\": [pgh.decode(g)[1] for g in all_geohash],\n",
    "})\n",
    "\n",
    "gh_gdf = gpd.GeoDataFrame(\n",
    "    gh_df,\n",
    "    geometry=gpd.points_from_xy(gh_df.lon, gh_df.lat),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "# spatial join ONCE\n",
    "gh_join = gpd.sjoin(\n",
    "    gh_gdf,\n",
    "    tracts,\n",
    "    how=\"left\",\n",
    "    predicate=\"intersects\"\n",
    ").drop(columns=[c for c in gh_gdf.columns if c.startswith(\"index_\")])\n",
    "\n",
    "geohash2tract = dict(\n",
    "    zip(gh_join[\"geohash\"], gh_join[TRACT_COL])\n",
    ")\n",
    "\n",
    "print(\"Geohash → tract lookup built.\")\n",
    "\n",
    "# =========================\n",
    "# Main OD container\n",
    "# =========================\n",
    "all_months = []\n",
    "\n",
    "# =========================\n",
    "# Loop months (NO sjoin here)\n",
    "# =========================\n",
    "for folder in tqdm(os.listdir(DELIVERY_ROOT), desc=\"Processing months\"):\n",
    "    if not folder.startswith(\"Salt_Lake-\"):\n",
    "        continue\n",
    "\n",
    "    month_dir = os.path.join(DELIVERY_ROOT, folder)\n",
    "\n",
    "    dfs = [\n",
    "        pd.read_parquet(os.path.join(month_dir, f))\n",
    "        for f in os.listdir(month_dir)\n",
    "        if f.endswith(\".parquet\")\n",
    "    ]\n",
    "    if not dfs:\n",
    "        continue\n",
    "\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # =========================\n",
    "    # Basic cleaning\n",
    "    # =========================\n",
    "    df = df[\n",
    "        df[\"geohash7_orig\"].notna() &\n",
    "        df[\"geohash7_dest\"].notna()\n",
    "    ].copy()\n",
    "\n",
    "    df[\"local_datetime_start\"] = pd.to_datetime(df[\"local_datetime_start\"])\n",
    "    df[\"month\"] = df[\"local_datetime_start\"].dt.to_period(\"M\").astype(str)\n",
    "    df[\"trip_weight\"] = df[\"trip_weight\"].fillna(1.0)\n",
    "\n",
    "    # =========================\n",
    "    # Map geohash → tract (FAST)\n",
    "    # =========================\n",
    "    df[\"origin_tract\"] = df[\"geohash7_orig\"].map(geohash2tract)\n",
    "    df[\"destination_tract\"] = df[\"geohash7_dest\"].map(geohash2tract)\n",
    "\n",
    "    df = df[\n",
    "        df[\"origin_tract\"].notna() &\n",
    "        df[\"destination_tract\"].notna()\n",
    "    ]\n",
    "\n",
    "    # =========================\n",
    "    # -------- UNLINKED OD --------\n",
    "    # =========================\n",
    "    unlinked_od = (\n",
    "        df\n",
    "        .groupby(\n",
    "            [\"month\", \"origin_tract\", \"destination_tract\", \"travel_mode\"],\n",
    "            as_index=False\n",
    "        )\n",
    "        .agg(\n",
    "            unlinked_count=(\"trip_id\", \"count\"),\n",
    "            unlinked_weighted_flow=(\"trip_weight\", \"sum\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # =========================\n",
    "    # -------- LINKED OD --------\n",
    "    # =========================\n",
    "    df = df.sort_values([\"linked_trip_id\", \"local_datetime_start\"])\n",
    "\n",
    "    linked_base = (\n",
    "        df\n",
    "        .groupby(\"linked_trip_id\", as_index=False)\n",
    "        .agg(\n",
    "            month=(\"month\", \"first\"),\n",
    "            travel_mode=(\"travel_mode\", \"first\"),\n",
    "            origin_tract=(\"origin_tract\", \"first\"),\n",
    "            destination_tract=(\"destination_tract\", \"last\"),\n",
    "            linked_weight=(\"trip_weight\", \"first\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    linked_od = (\n",
    "        linked_base\n",
    "        .groupby(\n",
    "            [\"month\", \"origin_tract\", \"destination_tract\", \"travel_mode\"],\n",
    "            as_index=False\n",
    "        )\n",
    "        .agg(\n",
    "            linked_count=(\"linked_trip_id\", \"count\"),\n",
    "            linked_weighted_flow=(\"linked_weight\", \"sum\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # =========================\n",
    "    # Merge linked + unlinked\n",
    "    # =========================\n",
    "    od = pd.merge(\n",
    "        unlinked_od,\n",
    "        linked_od,\n",
    "        on=[\"month\", \"origin_tract\", \"destination_tract\", \"travel_mode\"],\n",
    "        how=\"outer\"\n",
    "    )\n",
    "\n",
    "    all_months.append(od)\n",
    "\n",
    "# =========================\n",
    "# Final output\n",
    "# =========================\n",
    "final_df = pd.concat(all_months, ignore_index=True).fillna(0)\n",
    "\n",
    "final_df[[\"origin_tract\", \"destination_tract\"]] = (\n",
    "    final_df[[\"origin_tract\", \"destination_tract\"]].astype(str)\n",
    ")\n",
    "\n",
    "with open(OUT_OD_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(final_df.to_dict(orient=\"records\"), f, indent=2)\n",
    "\n",
    "print(f\"Saved tract-level OD → {OUT_OD_JSON}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9c4466c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building TOP-20 OD for dashboard (excluding self-loops)...\n",
      "Removed self-loop OD rows: 0\n",
      "Dashboard OD rows after TOP-20: 240\n",
      "Removed OD rows with invalid centroid: 0\n",
      "Saved dashboard TOP-20 OD with coordinates (no self-loops, safe) → ./od_dashboard_topk.json\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Build DASHBOARD-level TOP-K OD (WITH COORDINATES) — FINAL\n",
    "# =========================\n",
    "\n",
    "TOP_K = 20   # 推荐 20；10 会太稀疏\n",
    "\n",
    "print(f\"Building TOP-{TOP_K} OD for dashboard (excluding self-loops)...\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 1️⃣ 计算总 flow（排序指标）\n",
    "# -------------------------------------------------\n",
    "final_df = final_df.copy()\n",
    "\n",
    "final_df[\"total_flow\"] = (\n",
    "    final_df[\"linked_weighted_flow\"].fillna(0) +\n",
    "    final_df[\"unlinked_weighted_flow\"].fillna(0)\n",
    ")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2️⃣ 过滤：去掉同 O 同 D（自环）\n",
    "# -------------------------------------------------\n",
    "before_cnt = len(final_df)\n",
    "\n",
    "final_df = final_df[\n",
    "    final_df[\"origin_tract\"] != final_df[\"destination_tract\"]\n",
    "]\n",
    "\n",
    "print(f\"Removed self-loop OD rows: {before_cnt - len(final_df)}\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3️⃣ 按 month 选 TOP-K\n",
    "# -------------------------------------------------\n",
    "topk_df = (\n",
    "    final_df\n",
    "    .sort_values(\"total_flow\", ascending=False)\n",
    "    .groupby(\"month\", group_keys=False)\n",
    "    .head(TOP_K)\n",
    ")\n",
    "\n",
    "print(f\"Dashboard OD rows after TOP-{TOP_K}: {len(topk_df)}\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4️⃣ 加载 tract centroids（一次）\n",
    "# -------------------------------------------------\n",
    "with open(OUT_TRACT_CENTROID_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    tract_centroids = json.load(f)\n",
    "\n",
    "centroid_df = (\n",
    "    pd.DataFrame\n",
    "    .from_dict(tract_centroids, orient=\"index\")\n",
    "    .rename(columns={\"lat\": \"lat\", \"lon\": \"lon\"})\n",
    ")\n",
    "\n",
    "centroid_df.index.name = \"tract\"\n",
    "centroid_df.reset_index(inplace=True)\n",
    "centroid_df[\"tract\"] = centroid_df[\"tract\"].astype(str)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 5️⃣ 向量化 join 坐标（快 & 稳）\n",
    "# -------------------------------------------------\n",
    "topk_df[\"origin_tract\"] = topk_df[\"origin_tract\"].astype(str)\n",
    "topk_df[\"destination_tract\"] = topk_df[\"destination_tract\"].astype(str)\n",
    "\n",
    "topk_df = (\n",
    "    topk_df\n",
    "    .merge(\n",
    "        centroid_df.rename(columns={\n",
    "            \"tract\": \"origin_tract\",\n",
    "            \"lat\": \"o_lat\",\n",
    "            \"lon\": \"o_lon\"\n",
    "        }),\n",
    "        on=\"origin_tract\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    .merge(\n",
    "        centroid_df.rename(columns={\n",
    "            \"tract\": \"destination_tract\",\n",
    "            \"lat\": \"d_lat\",\n",
    "            \"lon\": \"d_lon\"\n",
    "        }),\n",
    "        on=\"destination_tract\",\n",
    "        how=\"left\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 6️⃣ 严格清洗非法坐标（关键）\n",
    "# -------------------------------------------------\n",
    "before_geo = len(topk_df)\n",
    "\n",
    "topk_df = topk_df.dropna(\n",
    "    subset=[\"o_lat\", \"o_lon\", \"d_lat\", \"d_lon\"]\n",
    ")\n",
    "\n",
    "# 防 NaN / inf / 非数值\n",
    "for c in [\"o_lat\", \"o_lon\", \"d_lat\", \"d_lon\"]:\n",
    "    topk_df = topk_df[pd.to_numeric(topk_df[c], errors=\"coerce\").notna()]\n",
    "\n",
    "print(f\"Removed OD rows with invalid centroid: {before_geo - len(topk_df)}\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 7️⃣ 只保留 Dashboard 必需字段\n",
    "# -------------------------------------------------\n",
    "dashboard_cols = [\n",
    "    \"month\",\n",
    "    \"origin_tract\",\n",
    "    \"destination_tract\",\n",
    "    \"o_lat\", \"o_lon\",\n",
    "    \"d_lat\", \"d_lon\",\n",
    "    \"travel_mode\",\n",
    "    \"linked_count\",\n",
    "    \"unlinked_count\",\n",
    "    \"linked_weighted_flow\",\n",
    "    \"unlinked_weighted_flow\",\n",
    "    \"total_flow\"\n",
    "]\n",
    "\n",
    "topk_df = topk_df[dashboard_cols]\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 8️⃣ 非坐标字段 fillna（坐标严禁填）\n",
    "# -------------------------------------------------\n",
    "count_cols = [\n",
    "    \"linked_count\",\n",
    "    \"unlinked_count\",\n",
    "    \"linked_weighted_flow\",\n",
    "    \"unlinked_weighted_flow\",\n",
    "    \"total_flow\"\n",
    "]\n",
    "\n",
    "topk_df[count_cols] = topk_df[count_cols].fillna(0)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 9️⃣ 输出 Dashboard JSON\n",
    "# -------------------------------------------------\n",
    "OUT_DASHBOARD_JSON = \"./od_dashboard_topk.json\"\n",
    "\n",
    "with open(OUT_DASHBOARD_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(\n",
    "        topk_df.to_dict(orient=\"records\"),\n",
    "        f,\n",
    "        indent=2\n",
    "    )\n",
    "\n",
    "print(\n",
    "    f\"Saved dashboard TOP-{TOP_K} OD with coordinates \"\n",
    "    f\"(no self-loops, safe) → {OUT_DASHBOARD_JSON}\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
